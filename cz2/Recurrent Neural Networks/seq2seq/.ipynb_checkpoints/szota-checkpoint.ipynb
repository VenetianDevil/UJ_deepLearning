{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 30000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "# Data downloaded from http://www.manythings.org/anki/\n",
    "# File: pol-eng.zip\n",
    "data_path = 'C:\\\\Users\\\\Default\\\\pol.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45375\n",
      "10000 10000\n",
      "Go.\n",
      "START_ Idź. _END\n",
      "['Go.', 'Hi.', 'Run!', 'Run.', 'Run.']\n",
      "['START_ Idź. _END', 'START_ Cześć. _END', 'START_ Uciekaj! _END', 'START_ Biegnij. _END', 'START_ Uciekaj. _END']\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "#for line in lines[: min(len(lines), len(lines) - 1)]:\n",
    "    input_text, target_text, ignore = line.split('\\t')\n",
    "    #\"START_\" as the \"start sequence\" character\n",
    "    #\"_END\" as \"end sequence\" character.\"\n",
    "    target_text = 'START_ ' + target_text + ' _END'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4594\n",
      "8060\n"
     ]
    }
   ],
   "source": [
    "#Vocabularies for English and Polish\n",
    "all_eng_words=set()\n",
    "for eng in input_texts:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "            \n",
    "all_pol_words=set()\n",
    "for pol in target_texts:\n",
    "    for word in pol.split():\n",
    "        if word not in all_pol_words:\n",
    "            all_pol_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4594 8061\n"
     ]
    }
   ],
   "source": [
    "#Unique number of tokens\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_pol_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_pol_words)\n",
    "num_decoder_tokens += 1 #zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "#Max length of sequences\n",
    "length_list_encoder=[]\n",
    "length_list_decoder=[]\n",
    "\n",
    "for l in input_texts:\n",
    "    length_list_encoder.append(len(l.split(' ')))\n",
    "max_encoder_seq_length = np.max(length_list_encoder)\n",
    "\n",
    "for l in target_texts:\n",
    "    length_list_decoder.append(len(l.split(' ')))\n",
    "max_decoder_seq_length = np.max(length_list_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionaries word->token and vice versa\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_word_index = dict(\n",
    "    (i, word) for word, i in input_token_index.items())\n",
    "reverse_target_word_index = dict(\n",
    "    (i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go.', 'START_ Idź. _END'), ('Hi.', 'START_ Cześć. _END'), ('Run!', 'START_ Uciekaj! _END'), ('Run.', 'START_ Biegnij. _END'), ('Run.', 'START_ Uciekaj. _END')]\n",
      "('Come immediately.', 'I am short.', 'Please look for it.', \"I'm here.\", 'They struggled.')\n",
      "('START_ Przyjdź natychmiast. _END', 'START_ Jestem niski. _END', 'START_ Proszę poszukaj tego. _END', 'START_ Jestem tutaj. _END', 'START_ Walczyli. _END')\n"
     ]
    }
   ],
   "source": [
    "#Shuffle data\n",
    "both_texts = list(zip(input_texts, target_texts))\n",
    "random.shuffle(both_texts)\n",
    "input_texts, target_texts = zip(*both_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 9000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "#Split data to train and test\n",
    "x, y = input_texts, target_texts\n",
    "x_train, y_train = input_texts[:int(len(input_texts)*0.9)], target_texts[:int(len(target_texts)*0.9)]\n",
    "x_test, y_test = input_texts[int(len(input_texts)*0.9):], target_texts[int(len(target_texts)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare encoding for the data\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length),dtype='float32')\n",
    "decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding_layer = Embedding(num_decoder_tokens, latent_dim)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, None, 256)    1176064     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 256)    2063616     input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 256), (None, 525312      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 256),  525312      embedding_10[0][0]               \n",
      "                                                                 lstm_9[0][1]                     \n",
      "                                                                 lstm_9[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 8061)   2071677     lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,361,981\n",
      "Trainable params: 6,361,981\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konrad\\mambaforge\\envs\\tfgpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "9000/9000 [==============================] - 22s 2ms/step - loss: 1.9776 - acc: 0.1012 - val_loss: 1.8827 - val_acc: 0.1073\n",
      "Epoch 2/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.6964 - acc: 0.1161 - val_loss: 1.7441 - val_acc: 0.1188\n",
      "Epoch 3/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.5842 - acc: 0.1262 - val_loss: 1.6924 - val_acc: 0.1277\n",
      "Epoch 4/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 1.5077 - acc: 0.1350 - val_loss: 1.6693 - val_acc: 0.1322\n",
      "Epoch 5/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.4385 - acc: 0.1433 - val_loss: 1.6048 - val_acc: 0.1424\n",
      "Epoch 6/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.3762 - acc: 0.1502 - val_loss: 1.5852 - val_acc: 0.1455\n",
      "Epoch 7/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.3164 - acc: 0.1563 - val_loss: 1.5737 - val_acc: 0.1464\n",
      "Epoch 8/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.2626 - acc: 0.1617 - val_loss: 1.5662 - val_acc: 0.1468\n",
      "Epoch 9/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.2149 - acc: 0.1661 - val_loss: 1.5629 - val_acc: 0.1444\n",
      "Epoch 10/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.1680 - acc: 0.1708 - val_loss: 1.5715 - val_acc: 0.1455\n",
      "Epoch 11/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.1265 - acc: 0.1745 - val_loss: 1.5916 - val_acc: 0.1380\n",
      "Epoch 12/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.0897 - acc: 0.1788 - val_loss: 1.5860 - val_acc: 0.1409\n",
      "Epoch 13/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.0543 - acc: 0.1829 - val_loss: 1.5896 - val_acc: 0.1395\n",
      "Epoch 14/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 1.0242 - acc: 0.1864 - val_loss: 1.6131 - val_acc: 0.1396\n",
      "Epoch 15/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.9943 - acc: 0.1899 - val_loss: 1.6025 - val_acc: 0.1420\n",
      "Epoch 16/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.9626 - acc: 0.1940 - val_loss: 1.6207 - val_acc: 0.1392\n",
      "Epoch 17/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.9345 - acc: 0.1973 - val_loss: 1.5946 - val_acc: 0.1424\n",
      "Epoch 18/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.9088 - acc: 0.2013 - val_loss: 1.6230 - val_acc: 0.1396\n",
      "Epoch 19/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.8856 - acc: 0.2043 - val_loss: 1.6314 - val_acc: 0.1421\n",
      "Epoch 20/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.8619 - acc: 0.2078 - val_loss: 1.6361 - val_acc: 0.1415\n",
      "Epoch 21/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.8396 - acc: 0.2117 - val_loss: 1.6369 - val_acc: 0.1445\n",
      "Epoch 22/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.8206 - acc: 0.2141 - val_loss: 1.6740 - val_acc: 0.1401\n",
      "Epoch 23/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.7979 - acc: 0.2174 - val_loss: 1.6641 - val_acc: 0.1435\n",
      "Epoch 24/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.7801 - acc: 0.2208 - val_loss: 1.6644 - val_acc: 0.1440\n",
      "Epoch 25/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.7611 - acc: 0.2235 - val_loss: 1.6566 - val_acc: 0.1446\n",
      "Epoch 26/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.7453 - acc: 0.2268 - val_loss: 1.6905 - val_acc: 0.1449\n",
      "Epoch 27/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.7284 - acc: 0.2295 - val_loss: 1.6795 - val_acc: 0.1435\n",
      "Epoch 28/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.7102 - acc: 0.2317 - val_loss: 1.6748 - val_acc: 0.1452\n",
      "Epoch 29/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.6932 - acc: 0.2346 - val_loss: 1.7030 - val_acc: 0.1440\n",
      "Epoch 30/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.6784 - acc: 0.2374 - val_loss: 1.7034 - val_acc: 0.1441\n",
      "Epoch 31/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.6651 - acc: 0.2395 - val_loss: 1.7172 - val_acc: 0.1455\n",
      "Epoch 32/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.6532 - acc: 0.2420 - val_loss: 1.7180 - val_acc: 0.1450\n",
      "Epoch 33/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.6429 - acc: 0.2445 - val_loss: 1.7289 - val_acc: 0.1458\n",
      "Epoch 34/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.6333 - acc: 0.2465 - val_loss: 1.7202 - val_acc: 0.1451\n",
      "Epoch 35/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.6222 - acc: 0.2485 - val_loss: 1.7325 - val_acc: 0.1461\n",
      "Epoch 36/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.6130 - acc: 0.2507 - val_loss: 1.7484 - val_acc: 0.1460\n",
      "Epoch 37/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.6061 - acc: 0.2521 - val_loss: 1.7385 - val_acc: 0.1461\n",
      "Epoch 38/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5974 - acc: 0.2543 - val_loss: 1.7459 - val_acc: 0.1465\n",
      "Epoch 39/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5898 - acc: 0.2558 - val_loss: 1.7489 - val_acc: 0.1465\n",
      "Epoch 40/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5837 - acc: 0.2569 - val_loss: 1.7575 - val_acc: 0.1468\n",
      "Epoch 41/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5774 - acc: 0.2583 - val_loss: 1.7733 - val_acc: 0.1460\n",
      "Epoch 42/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5708 - acc: 0.2594 - val_loss: 1.7782 - val_acc: 0.1461\n",
      "Epoch 43/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5654 - acc: 0.2608 - val_loss: 1.7815 - val_acc: 0.1457\n",
      "Epoch 44/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5606 - acc: 0.2618 - val_loss: 1.7915 - val_acc: 0.1456\n",
      "Epoch 45/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5565 - acc: 0.2627 - val_loss: 1.8037 - val_acc: 0.1455\n",
      "Epoch 46/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5525 - acc: 0.2635 - val_loss: 1.8062 - val_acc: 0.1452\n",
      "Epoch 47/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5483 - acc: 0.2645 - val_loss: 1.8086 - val_acc: 0.1453\n",
      "Epoch 48/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5454 - acc: 0.2648 - val_loss: 1.8081 - val_acc: 0.1455\n",
      "Epoch 49/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5423 - acc: 0.2657 - val_loss: 1.8095 - val_acc: 0.1455\n",
      "Epoch 50/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5405 - acc: 0.2656 - val_loss: 1.8247 - val_acc: 0.1457\n",
      "Epoch 51/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.5369 - acc: 0.2668 - val_loss: 1.8220 - val_acc: 0.1450\n",
      "Epoch 52/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5341 - acc: 0.2674 - val_loss: 1.8220 - val_acc: 0.1460\n",
      "Epoch 53/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5319 - acc: 0.2677 - val_loss: 1.8326 - val_acc: 0.1447\n",
      "Epoch 54/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.5297 - acc: 0.2682 - val_loss: 1.8354 - val_acc: 0.1460\n",
      "Epoch 55/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5273 - acc: 0.2691 - val_loss: 1.8329 - val_acc: 0.1454\n",
      "Epoch 56/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5252 - acc: 0.2693 - val_loss: 1.8361 - val_acc: 0.1458\n",
      "Epoch 57/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5236 - acc: 0.2696 - val_loss: 1.8472 - val_acc: 0.1455\n",
      "Epoch 58/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5211 - acc: 0.2702 - val_loss: 1.8505 - val_acc: 0.1458\n",
      "Epoch 59/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5198 - acc: 0.2705 - val_loss: 1.8577 - val_acc: 0.1460\n",
      "Epoch 60/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5177 - acc: 0.2712 - val_loss: 1.8534 - val_acc: 0.1451\n",
      "Epoch 61/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5168 - acc: 0.2710 - val_loss: 1.8612 - val_acc: 0.1468\n",
      "Epoch 62/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5145 - acc: 0.2719 - val_loss: 1.8606 - val_acc: 0.1465\n",
      "Epoch 63/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5131 - acc: 0.2722 - val_loss: 1.8702 - val_acc: 0.1455\n",
      "Epoch 64/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5132 - acc: 0.2719 - val_loss: 1.8750 - val_acc: 0.1457\n",
      "Epoch 65/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5118 - acc: 0.2725 - val_loss: 1.8750 - val_acc: 0.1456\n",
      "Epoch 66/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5107 - acc: 0.2725 - val_loss: 1.8735 - val_acc: 0.1465\n",
      "Epoch 67/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5104 - acc: 0.2728 - val_loss: 1.8782 - val_acc: 0.1465\n",
      "Epoch 68/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5093 - acc: 0.2731 - val_loss: 1.8825 - val_acc: 0.1456\n",
      "Epoch 69/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5090 - acc: 0.2730 - val_loss: 1.8818 - val_acc: 0.1450\n",
      "Epoch 70/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5085 - acc: 0.2733 - val_loss: 1.8871 - val_acc: 0.1455\n",
      "Epoch 71/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5079 - acc: 0.2736 - val_loss: 1.8901 - val_acc: 0.1459\n",
      "Epoch 72/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5073 - acc: 0.2743 - val_loss: 1.8967 - val_acc: 0.1452\n",
      "Epoch 73/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5080 - acc: 0.2741 - val_loss: 1.9033 - val_acc: 0.1439\n",
      "Epoch 74/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5070 - acc: 0.2744 - val_loss: 1.9033 - val_acc: 0.1449\n",
      "Epoch 75/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5071 - acc: 0.2745 - val_loss: 1.9043 - val_acc: 0.1458\n",
      "Epoch 76/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5081 - acc: 0.2743 - val_loss: 1.9111 - val_acc: 0.1452\n",
      "Epoch 77/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5085 - acc: 0.2742 - val_loss: 1.9129 - val_acc: 0.1445\n",
      "Epoch 78/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5081 - acc: 0.2742 - val_loss: 1.9184 - val_acc: 0.1460\n",
      "Epoch 79/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5090 - acc: 0.2746 - val_loss: 1.9207 - val_acc: 0.1445\n",
      "Epoch 80/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5091 - acc: 0.2746 - val_loss: 1.9224 - val_acc: 0.1450\n",
      "Epoch 81/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5087 - acc: 0.2748 - val_loss: 1.9281 - val_acc: 0.1452\n",
      "Epoch 82/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5088 - acc: 0.2753 - val_loss: 1.9323 - val_acc: 0.1449\n",
      "Epoch 83/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5094 - acc: 0.2749 - val_loss: 1.9306 - val_acc: 0.1454\n",
      "Epoch 84/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5087 - acc: 0.2753 - val_loss: 1.9399 - val_acc: 0.1462\n",
      "Epoch 85/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5082 - acc: 0.2754 - val_loss: 1.9340 - val_acc: 0.1455\n",
      "Epoch 86/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5088 - acc: 0.2750 - val_loss: 1.9406 - val_acc: 0.1450\n",
      "Epoch 87/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5086 - acc: 0.2754 - val_loss: 1.9423 - val_acc: 0.1459\n",
      "Epoch 88/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5084 - acc: 0.2751 - val_loss: 1.9512 - val_acc: 0.1443\n",
      "Epoch 89/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5071 - acc: 0.2756 - val_loss: 1.9545 - val_acc: 0.1463\n",
      "Epoch 90/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5067 - acc: 0.2751 - val_loss: 1.9571 - val_acc: 0.1462\n",
      "Epoch 91/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5071 - acc: 0.2753 - val_loss: 1.9587 - val_acc: 0.1451\n",
      "Epoch 92/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5078 - acc: 0.2752 - val_loss: 1.9622 - val_acc: 0.1446\n",
      "Epoch 93/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5087 - acc: 0.2755 - val_loss: 1.9635 - val_acc: 0.1448\n",
      "Epoch 94/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5084 - acc: 0.2755 - val_loss: 1.9664 - val_acc: 0.1450\n",
      "Epoch 95/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5088 - acc: 0.2754 - val_loss: 1.9668 - val_acc: 0.1455\n",
      "Epoch 96/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5086 - acc: 0.2754 - val_loss: 1.9753 - val_acc: 0.1454\n",
      "Epoch 97/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5076 - acc: 0.2759 - val_loss: 1.9742 - val_acc: 0.1442\n",
      "Epoch 98/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5079 - acc: 0.2751 - val_loss: 1.9820 - val_acc: 0.1458\n",
      "Epoch 99/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5086 - acc: 0.2756 - val_loss: 1.9744 - val_acc: 0.1446\n",
      "Epoch 100/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5083 - acc: 0.2753 - val_loss: 1.9771 - val_acc: 0.1455\n",
      "Epoch 101/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5089 - acc: 0.2756 - val_loss: 1.9803 - val_acc: 0.1443\n",
      "Epoch 102/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5083 - acc: 0.2755 - val_loss: 1.9854 - val_acc: 0.1448\n",
      "Epoch 103/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5082 - acc: 0.2754 - val_loss: 1.9869 - val_acc: 0.1452\n",
      "Epoch 104/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5075 - acc: 0.2756 - val_loss: 1.9820 - val_acc: 0.1457\n",
      "Epoch 105/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5069 - acc: 0.2753 - val_loss: 1.9898 - val_acc: 0.1444\n",
      "Epoch 106/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5067 - acc: 0.2758 - val_loss: 1.9904 - val_acc: 0.1449\n",
      "Epoch 107/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5060 - acc: 0.2755 - val_loss: 1.9894 - val_acc: 0.1440\n",
      "Epoch 108/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5057 - acc: 0.2757 - val_loss: 1.9942 - val_acc: 0.1445\n",
      "Epoch 109/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5049 - acc: 0.2760 - val_loss: 1.9996 - val_acc: 0.1445\n",
      "Epoch 110/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5050 - acc: 0.2759 - val_loss: 1.9976 - val_acc: 0.1445\n",
      "Epoch 111/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5041 - acc: 0.2759 - val_loss: 1.9959 - val_acc: 0.1464\n",
      "Epoch 112/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5044 - acc: 0.2761 - val_loss: 1.9952 - val_acc: 0.1445\n",
      "Epoch 113/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5032 - acc: 0.2762 - val_loss: 2.0031 - val_acc: 0.1452\n",
      "Epoch 114/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5030 - acc: 0.2765 - val_loss: 2.0016 - val_acc: 0.1445\n",
      "Epoch 115/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5029 - acc: 0.2761 - val_loss: 2.0032 - val_acc: 0.1447\n",
      "Epoch 116/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5030 - acc: 0.2764 - val_loss: 2.0037 - val_acc: 0.1452\n",
      "Epoch 117/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5026 - acc: 0.2765 - val_loss: 2.0075 - val_acc: 0.1461\n",
      "Epoch 118/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5022 - acc: 0.2761 - val_loss: 2.0116 - val_acc: 0.1449\n",
      "Epoch 119/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.5020 - acc: 0.2761 - val_loss: 2.0099 - val_acc: 0.1458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5012 - acc: 0.2765 - val_loss: 2.0107 - val_acc: 0.1454\n",
      "Epoch 121/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.5005 - acc: 0.2768 - val_loss: 2.0113 - val_acc: 0.1447\n",
      "Epoch 122/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.5001 - acc: 0.2770 - val_loss: 2.0141 - val_acc: 0.1443\n",
      "Epoch 123/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4995 - acc: 0.2769 - val_loss: 2.0179 - val_acc: 0.1449\n",
      "Epoch 124/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4996 - acc: 0.2767 - val_loss: 2.0213 - val_acc: 0.1450\n",
      "Epoch 125/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4984 - acc: 0.2769 - val_loss: 2.0140 - val_acc: 0.1445\n",
      "Epoch 126/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4978 - acc: 0.2772 - val_loss: 2.0198 - val_acc: 0.1448\n",
      "Epoch 127/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4977 - acc: 0.2774 - val_loss: 2.0216 - val_acc: 0.1461\n",
      "Epoch 128/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4991 - acc: 0.2769 - val_loss: 2.0211 - val_acc: 0.1447\n",
      "Epoch 129/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4979 - acc: 0.2770 - val_loss: 2.0220 - val_acc: 0.1448\n",
      "Epoch 130/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4979 - acc: 0.2766 - val_loss: 2.0236 - val_acc: 0.1434\n",
      "Epoch 131/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4976 - acc: 0.2767 - val_loss: 2.0245 - val_acc: 0.1447\n",
      "Epoch 132/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4975 - acc: 0.2766 - val_loss: 2.0298 - val_acc: 0.1460\n",
      "Epoch 133/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4960 - acc: 0.2768 - val_loss: 2.0231 - val_acc: 0.1440\n",
      "Epoch 134/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4948 - acc: 0.2773 - val_loss: 2.0263 - val_acc: 0.1451\n",
      "Epoch 135/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4955 - acc: 0.2769 - val_loss: 2.0329 - val_acc: 0.1448\n",
      "Epoch 136/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4948 - acc: 0.2777 - val_loss: 2.0319 - val_acc: 0.1440\n",
      "Epoch 137/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4947 - acc: 0.2770 - val_loss: 2.0261 - val_acc: 0.1453\n",
      "Epoch 138/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4947 - acc: 0.2772 - val_loss: 2.0358 - val_acc: 0.1447\n",
      "Epoch 139/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4930 - acc: 0.2777 - val_loss: 2.0399 - val_acc: 0.1455\n",
      "Epoch 140/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4927 - acc: 0.2773 - val_loss: 2.0369 - val_acc: 0.1455\n",
      "Epoch 141/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4933 - acc: 0.2775 - val_loss: 2.0380 - val_acc: 0.1448\n",
      "Epoch 142/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4932 - acc: 0.2773 - val_loss: 2.0396 - val_acc: 0.1445\n",
      "Epoch 143/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4916 - acc: 0.2777 - val_loss: 2.0442 - val_acc: 0.1455\n",
      "Epoch 144/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4915 - acc: 0.2774 - val_loss: 2.0381 - val_acc: 0.1445\n",
      "Epoch 145/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4923 - acc: 0.2775 - val_loss: 2.0449 - val_acc: 0.1454\n",
      "Epoch 146/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4913 - acc: 0.2776 - val_loss: 2.0449 - val_acc: 0.1450\n",
      "Epoch 147/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4920 - acc: 0.2776 - val_loss: 2.0402 - val_acc: 0.1459\n",
      "Epoch 148/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4914 - acc: 0.2778 - val_loss: 2.0391 - val_acc: 0.1454\n",
      "Epoch 149/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4917 - acc: 0.2775 - val_loss: 2.0441 - val_acc: 0.1446\n",
      "Epoch 150/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4911 - acc: 0.2774 - val_loss: 2.0444 - val_acc: 0.1447\n",
      "Epoch 151/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4916 - acc: 0.2772 - val_loss: 2.0423 - val_acc: 0.1442\n",
      "Epoch 152/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4902 - acc: 0.2778 - val_loss: 2.0498 - val_acc: 0.1454\n",
      "Epoch 153/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4901 - acc: 0.2777 - val_loss: 2.0476 - val_acc: 0.1441\n",
      "Epoch 154/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4895 - acc: 0.2777 - val_loss: 2.0482 - val_acc: 0.1453\n",
      "Epoch 155/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4899 - acc: 0.2777 - val_loss: 2.0505 - val_acc: 0.1448\n",
      "Epoch 156/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4893 - acc: 0.2777 - val_loss: 2.0490 - val_acc: 0.1440\n",
      "Epoch 157/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4888 - acc: 0.2781 - val_loss: 2.0511 - val_acc: 0.1445\n",
      "Epoch 158/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4899 - acc: 0.2780 - val_loss: 2.0497 - val_acc: 0.1454\n",
      "Epoch 159/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4890 - acc: 0.2779 - val_loss: 2.0547 - val_acc: 0.1454\n",
      "Epoch 160/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4887 - acc: 0.2780 - val_loss: 2.0535 - val_acc: 0.1444\n",
      "Epoch 161/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4880 - acc: 0.2783 - val_loss: 2.0550 - val_acc: 0.1439\n",
      "Epoch 162/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4880 - acc: 0.2778 - val_loss: 2.0618 - val_acc: 0.1445\n",
      "Epoch 163/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4877 - acc: 0.2782 - val_loss: 2.0548 - val_acc: 0.1435\n",
      "Epoch 164/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4878 - acc: 0.2778 - val_loss: 2.0552 - val_acc: 0.1435\n",
      "Epoch 165/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4875 - acc: 0.2781 - val_loss: 2.0599 - val_acc: 0.1441\n",
      "Epoch 166/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4869 - acc: 0.2785 - val_loss: 2.0637 - val_acc: 0.1446\n",
      "Epoch 167/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4860 - acc: 0.2782 - val_loss: 2.0602 - val_acc: 0.1453\n",
      "Epoch 168/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4854 - acc: 0.2786 - val_loss: 2.0574 - val_acc: 0.1436\n",
      "Epoch 169/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4845 - acc: 0.2785 - val_loss: 2.0587 - val_acc: 0.1444\n",
      "Epoch 170/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4857 - acc: 0.2784 - val_loss: 2.0643 - val_acc: 0.1435\n",
      "Epoch 171/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4852 - acc: 0.2784 - val_loss: 2.0624 - val_acc: 0.1429\n",
      "Epoch 172/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4856 - acc: 0.2782 - val_loss: 2.0627 - val_acc: 0.1436\n",
      "Epoch 173/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4843 - acc: 0.2785 - val_loss: 2.0637 - val_acc: 0.1435\n",
      "Epoch 174/200\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 0.4846 - acc: 0.2784 - val_loss: 2.0637 - val_acc: 0.1443\n",
      "Epoch 175/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4842 - acc: 0.2784 - val_loss: 2.0626 - val_acc: 0.1444\n",
      "Epoch 176/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4845 - acc: 0.2785 - val_loss: 2.0663 - val_acc: 0.1449\n",
      "Epoch 177/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4843 - acc: 0.2782 - val_loss: 2.0692 - val_acc: 0.1444\n",
      "Epoch 178/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4840 - acc: 0.2782 - val_loss: 2.0644 - val_acc: 0.1453\n",
      "Epoch 179/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4847 - acc: 0.2780 - val_loss: 2.0659 - val_acc: 0.1435\n",
      "Epoch 180/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4841 - acc: 0.2781 - val_loss: 2.0670 - val_acc: 0.1443\n",
      "Epoch 181/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4833 - acc: 0.2784 - val_loss: 2.0734 - val_acc: 0.1435\n",
      "Epoch 182/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4824 - acc: 0.2781 - val_loss: 2.0683 - val_acc: 0.1438\n",
      "Epoch 183/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4823 - acc: 0.2786 - val_loss: 2.0730 - val_acc: 0.1434\n",
      "Epoch 184/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4821 - acc: 0.2789 - val_loss: 2.0726 - val_acc: 0.1438\n",
      "Epoch 185/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4819 - acc: 0.2790 - val_loss: 2.0714 - val_acc: 0.1442\n",
      "Epoch 186/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4813 - acc: 0.2787 - val_loss: 2.0663 - val_acc: 0.1451\n",
      "Epoch 187/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4820 - acc: 0.2783 - val_loss: 2.0732 - val_acc: 0.1428\n",
      "Epoch 188/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4818 - acc: 0.2785 - val_loss: 2.0679 - val_acc: 0.1444\n",
      "Epoch 189/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4814 - acc: 0.2787 - val_loss: 2.0758 - val_acc: 0.1443\n",
      "Epoch 190/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4806 - acc: 0.2788 - val_loss: 2.0787 - val_acc: 0.1442\n",
      "Epoch 191/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4814 - acc: 0.2786 - val_loss: 2.0751 - val_acc: 0.1439\n",
      "Epoch 192/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4808 - acc: 0.2787 - val_loss: 2.0767 - val_acc: 0.1447\n",
      "Epoch 193/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4811 - acc: 0.2786 - val_loss: 2.0803 - val_acc: 0.1435\n",
      "Epoch 194/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4809 - acc: 0.2786 - val_loss: 2.0765 - val_acc: 0.1441\n",
      "Epoch 195/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4806 - acc: 0.2788 - val_loss: 2.0834 - val_acc: 0.1427\n",
      "Epoch 196/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4808 - acc: 0.2788 - val_loss: 2.0815 - val_acc: 0.1433\n",
      "Epoch 197/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4810 - acc: 0.2786 - val_loss: 2.0869 - val_acc: 0.1440\n",
      "Epoch 198/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4813 - acc: 0.2788 - val_loss: 2.0837 - val_acc: 0.1424\n",
      "Epoch 199/200\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.4811 - acc: 0.2786 - val_loss: 2.0850 - val_acc: 0.1436\n",
      "Epoch 200/200\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.4813 - acc: 0.2788 - val_loss: 2.0800 - val_acc: 0.1432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x222856d3408>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define sampling models\n",
    "#Inference step\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedding_2 = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_outputs_2, state_h_2, state_c_2 = decoder_lstm(decoder_embedding_2, initial_state=decoder_states_inputs)\n",
    "decoder_states_2 = [state_h_2, state_c_2]\n",
    "\n",
    "decoder_outputs_2 = decoder_dense(decoder_outputs_2)\n",
    "\n",
    "#Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs_2] + decoder_states_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_word_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '_END' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length + 1):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Come immediately.\n",
      "Decoded sentence:  Przyjdź natychmiast.\n",
      "-\n",
      "Input sentence: I am short.\n",
      "Decoded sentence:  Jestem niski.\n",
      "-\n",
      "Input sentence: Please look for it.\n",
      "Decoded sentence:  Proszę Proszę\n",
      "-\n",
      "Input sentence: I'm here.\n",
      "Decoded sentence:  Jestem tutaj.\n",
      "-\n",
      "Input sentence: They struggled.\n",
      "Decoded sentence:  Oni tego Tom\n",
      "-\n",
      "Input sentence: Face facts!\n",
      "Decoded sentence:  Spójrz się z\n",
      "-\n",
      "Input sentence: I wrote it.\n",
      "Decoded sentence:  Napisałem to.\n",
      "-\n",
      "Input sentence: Do you watch movies?\n",
      "Decoded sentence:  Oglądasz Oglądasz\n",
      "-\n",
      "Input sentence: Tom is missing.\n",
      "Decoded sentence:  Tom zaginął.\n",
      "-\n",
      "Input sentence: Sit down, please.\n",
      "Decoded sentence:  Usiądź proszę.\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence:  Bądź uczciwy.\n",
      "-\n",
      "Input sentence: I eat cheese.\n",
      "Decoded sentence:  Jem ser. _END\n",
      "-\n",
      "Input sentence: Don't fight.\n",
      "Decoded sentence:  Nie walcz. _END\n",
      "-\n",
      "Input sentence: I'm in no hurry.\n",
      "Decoded sentence:  Nie Jestem w\n",
      "-\n",
      "Input sentence: Stop showing off!\n",
      "Decoded sentence:  Przestań się\n",
      "-\n",
      "Input sentence: It's snowing now.\n",
      "Decoded sentence:  Teraz pada śnieg.\n",
      "-\n",
      "Input sentence: He's a slacker.\n",
      "Decoded sentence:  Jest Jest na\n",
      "-\n",
      "Input sentence: I'm divorced.\n",
      "Decoded sentence:  Jestem rozwiedziony.\n",
      "-\n",
      "Input sentence: Don't move.\n",
      "Decoded sentence:  Nie ruszaj się.\n",
      "-\n",
      "Input sentence: Let's hurry up.\n",
      "Decoded sentence:  Pośpieszmy się.\n",
      "-\n",
      "Input sentence: I don't see him.\n",
      "Decoded sentence:  Nie widzę go.\n",
      "-\n",
      "Input sentence: Tom is intolerant.\n",
      "Decoded sentence:  Tom jest cię\n",
      "-\n",
      "Input sentence: I knit.\n",
      "Decoded sentence:  Nie drutach.\n",
      "-\n",
      "Input sentence: I'm buying a rose.\n",
      "Decoded sentence:  jest cię. _END\n",
      "-\n",
      "Input sentence: How absurd!\n",
      "Decoded sentence:  Ale absurd! _END\n",
      "-\n",
      "Input sentence: They started it.\n",
      "Decoded sentence:  Oni to zaczęli.\n",
      "-\n",
      "Input sentence: Why bother?\n",
      "Decoded sentence:  Po co się To\n",
      "-\n",
      "Input sentence: Tom barfed.\n",
      "Decoded sentence:  Tom zwymiotował.\n",
      "-\n",
      "Input sentence: You know I can't.\n",
      "Decoded sentence:  Powinieneś się\n",
      "-\n",
      "Input sentence: I'm very happy.\n",
      "Decoded sentence:  Jestem bardzo\n",
      "-\n",
      "Input sentence: I'm a lonely man.\n",
      "Decoded sentence:  Jestem człowiekiem.\n",
      "-\n",
      "Input sentence: Why do you ask?\n",
      "Decoded sentence:  Czemu pytasz?\n",
      "-\n",
      "Input sentence: We know.\n",
      "Decoded sentence:  jest na go. _END\n",
      "-\n",
      "Input sentence: Start counting.\n",
      "Decoded sentence:  Zacznij Zacznij\n",
      "-\n",
      "Input sentence: Tom was in jail.\n",
      "Decoded sentence:  Tom był w więzieniu.\n",
      "-\n",
      "Input sentence: Write something.\n",
      "Decoded sentence:  Napisz coś. _END\n",
      "-\n",
      "Input sentence: Who is she?\n",
      "Decoded sentence:  Kim ona jest?\n",
      "-\n",
      "Input sentence: Everyone slept.\n",
      "Decoded sentence:  Każdy spał. _END\n",
      "-\n",
      "Input sentence: That's our future.\n",
      "Decoded sentence:  To nasza nasza\n",
      "-\n",
      "Input sentence: Tom loves his work.\n",
      "Decoded sentence:  Tom lubi swoją\n",
      "-\n",
      "Input sentence: Jesus wept.\n",
      "Decoded sentence:  Czy to moim Nienawidzę\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence:  Dobrze Lubię\n",
      "-\n",
      "Input sentence: She looks young.\n",
      "Decoded sentence:  Ona wygląda młodo.\n",
      "-\n",
      "Input sentence: All were satisfied.\n",
      "Decoded sentence:  Wszyscy byli\n",
      "-\n",
      "Input sentence: We're going home.\n",
      "Decoded sentence:  się domu. _END\n",
      "-\n",
      "Input sentence: Nobody slept.\n",
      "Decoded sentence:  Nikt nie spał.\n",
      "-\n",
      "Input sentence: Evil always wins.\n",
      "Decoded sentence:  Oboje zawsze\n",
      "-\n",
      "Input sentence: Can you see Tom?\n",
      "Decoded sentence:  Czy możesz Toma?\n",
      "-\n",
      "Input sentence: We don't know her.\n",
      "Decoded sentence:  Nie znamy jej.\n",
      "-\n",
      "Input sentence: You're decisive.\n",
      "Decoded sentence:  Jesteś Jesteś\n",
      "-\n",
      "Input sentence: Do you like movies?\n",
      "Decoded sentence:  Lubisz filmy?\n",
      "-\n",
      "Input sentence: It's pretty cold.\n",
      "Decoded sentence:  Jest na jest\n",
      "-\n",
      "Input sentence: It is very short.\n",
      "Decoded sentence:  To jest bardzo\n",
      "-\n",
      "Input sentence: I bought that car.\n",
      "Decoded sentence:  Kupiłem tamten\n",
      "-\n",
      "Input sentence: Please hurry up!\n",
      "Decoded sentence:  Proszę pospiesz\n",
      "-\n",
      "Input sentence: Add a little milk.\n",
      "Decoded sentence:  Byłam trochę\n",
      "-\n",
      "Input sentence: You've won!\n",
      "Decoded sentence:  to Oni jest Bądź\n",
      "-\n",
      "Input sentence: You're stalling.\n",
      "Decoded sentence:  Grasz na gotowy.\n",
      "-\n",
      "Input sentence: Tom vanished.\n",
      "Decoded sentence:  Tom na On jest\n",
      "-\n",
      "Input sentence: They got married.\n",
      "Decoded sentence:  ślub. _END\n",
      "-\n",
      "Input sentence: Study hard.\n",
      "Decoded sentence:  Ucz się Ucz się\n",
      "-\n",
      "Input sentence: Can we come, too?\n",
      "Decoded sentence:  Czy możesz też\n",
      "-\n",
      "Input sentence: I looked.\n",
      "Decoded sentence:  Proszę Czy są\n",
      "-\n",
      "Input sentence: Tom burped.\n",
      "Decoded sentence:  Tom Co Proszę\n",
      "-\n",
      "Input sentence: That hurts.\n",
      "Decoded sentence:  To boli. _END\n",
      "-\n",
      "Input sentence: That was nifty.\n",
      "Decoded sentence:  To było się Tom\n",
      "-\n",
      "Input sentence: Everyone escaped.\n",
      "Decoded sentence:  Wszyscy Wszyscy\n",
      "-\n",
      "Input sentence: Tom exercises.\n",
      "Decoded sentence:  On On jest z\n",
      "-\n",
      "Input sentence: They are alone.\n",
      "Decoded sentence:  Oni są sami.\n",
      "-\n",
      "Input sentence: Say \"aah.\"\n",
      "Decoded sentence:  Powiedz Powiedz\n",
      "-\n",
      "Input sentence: How are you, Tom?\n",
      "Decoded sentence:  Jak się pan Tom?\n",
      "-\n",
      "Input sentence: Has he arrived yet?\n",
      "Decoded sentence:  Czy on już przyszedł?\n",
      "-\n",
      "Input sentence: Large, isn't it?\n",
      "Decoded sentence:  Nie jest za dużo.\n",
      "-\n",
      "Input sentence: Keep quiet.\n",
      "Decoded sentence:  Bądź cicho. _END\n",
      "-\n",
      "Input sentence: Tom was wrong.\n",
      "Decoded sentence:  Tom był w błędzie.\n",
      "-\n",
      "Input sentence: He looks tired.\n",
      "Decoded sentence:  On wygląda na\n",
      "-\n",
      "Input sentence: You're pessimistic.\n",
      "Decoded sentence:  Jesteś pesymistą.\n",
      "-\n",
      "Input sentence: Have courage.\n",
      "Decoded sentence:  Bądź odważny.\n",
      "-\n",
      "Input sentence: You are naughty.\n",
      "Decoded sentence:  Jesteś Toma.\n",
      "-\n",
      "Input sentence: Stop there.\n",
      "Decoded sentence:  Zatrzymaj się.\n",
      "-\n",
      "Input sentence: Don't sweat it.\n",
      "Decoded sentence:  Nie martw się\n",
      "-\n",
      "Input sentence: Tom's outside.\n",
      "Decoded sentence:  Tom jest na zewnątrz.\n",
      "-\n",
      "Input sentence: You're diplomatic.\n",
      "Decoded sentence:  Jesteś się do\n",
      "-\n",
      "Input sentence: Finish this.\n",
      "Decoded sentence:  Skończ to. _END\n",
      "-\n",
      "Input sentence: Let's eat.\n",
      "Decoded sentence:  się Nie Jest\n",
      "-\n",
      "Input sentence: Cross the bridge.\n",
      "Decoded sentence:  Przejdź przez\n",
      "-\n",
      "Input sentence: My leg hurts.\n",
      "Decoded sentence:  Boli mnie noga.\n",
      "-\n",
      "Input sentence: I am the same age.\n",
      "Decoded sentence:  Jestem w tym\n",
      "-\n",
      "Input sentence: You're loaded.\n",
      "Decoded sentence:  Jesteś Czy Jesteś\n",
      "-\n",
      "Input sentence: Eat and drink.\n",
      "Decoded sentence:  Jedz i Jedz i\n",
      "-\n",
      "Input sentence: I want to be happy.\n",
      "Decoded sentence:  Chcę być szczęśliwy.\n",
      "-\n",
      "Input sentence: I can't leave.\n",
      "Decoded sentence:  Nie mogę wyjść.\n",
      "-\n",
      "Input sentence: Tom snores.\n",
      "Decoded sentence:  Tom z On Nie\n",
      "-\n",
      "Input sentence: We're both insane.\n",
      "Decoded sentence:  Oboje jesteśmy\n",
      "-\n",
      "Input sentence: How can I help?\n",
      "Decoded sentence:  Jak mogę pomóc?\n",
      "-\n",
      "Input sentence: Where is the vodka?\n",
      "Decoded sentence:  Gdzie jest Oto\n",
      "-\n",
      "Input sentence: What was happening?\n",
      "Decoded sentence:  Co się jest z\n",
      "-\n",
      "Input sentence: I missed supper.\n",
      "Decoded sentence:  Nie To z nim.\n",
      "-\n",
      "Input sentence: They're amateurs.\n",
      "Decoded sentence:  Oni są Ona się\n",
      "-\n",
      "Input sentence: Eat healthily.\n",
      "Decoded sentence:  Jedz zdrowo.\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
