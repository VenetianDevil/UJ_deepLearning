{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence example in Keras (character-level).\n",
    "\n",
    "Source:\n",
    "https://keras.io/examples/lstm_seq2seq/\n",
    "\n",
    "References:\n",
    "- https://arxiv.org/abs/1409.3215\n",
    "- https://arxiv.org/abs/1406.1078\n",
    "\n",
    "This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short Polish sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download\n",
    "\n",
    "Lots of neat sentence pairs datasets. http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 200  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = '../lang_pairs/pol.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the sentences (input = English, target = Polish),\n",
    "and extract the character sets for each language.\n",
    "\n",
    "Target sequences get additional \"start sequence\" and \"end sequence\" characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_words = set()\n",
    "target_words = set()\n",
    "corpus = []\n",
    "# input_words.add(' ')\n",
    "# target_words.add(' ')\n",
    "# input_words.add('\\t')\n",
    "# target_words.add('\\t')\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    # line = line.lower()\n",
    "    input_text, target_text, ignore = line.split('\\t') \n",
    "    # input_text = re.sub(\"'\", '', input_text)\n",
    "    # target_text = re.sub(\"'\", '', target_text)\n",
    "    # input_text = re.sub(\",\", ' COMMA', input_text)\n",
    "    # target_text = re.sub(\",\", ' COMMA', target_text)\n",
    "    # input_text = re.sub(r'[?|!|.|\"|0-9]', r'', input_text)\n",
    "    # target_text = re.sub(r'[?|!|.|\"|0-9]', r'', target_text)\n",
    "\n",
    "    target_text = 'start_ ' + target_text + ' _end'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for word in input_text.split():\n",
    "        if word not in input_words:\n",
    "            input_words.add(word)\n",
    "    for word in target_text.split():\n",
    "        if word not in target_words:\n",
    "            target_words.add(word)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer1 = CountVectorizer(lowercase=False)\n",
    "# vectorizer2 = CountVectorizer(lowercase=False)\n",
    "\n",
    "# X = vectorizer1.fit_transform(input_texts)\n",
    "# Y = vectorizer2.fit_transform(target_texts)\n",
    "\n",
    "# input_words = vectorizer1.get_feature_names()\n",
    "# target_words = vectorizer2.get_feature_names()\n",
    "# # target_words.append(\"START_\")\n",
    "# # target_words.append(\"_END\")\n",
    "\n",
    "# input_words.append(\"i\")\n",
    "# input_words.append(\"a\")\n",
    "# input_words.append(\"COMMA\")\n",
    "# input_words.append(\":\")\n",
    "# input_words.append(\"-\")\n",
    "# input_words.append(\"b\")\n",
    "\n",
    "# target_words.append(\"i\")\n",
    "# target_words.append(\"a\")\n",
    "# target_words.append(\"w\")\n",
    "# target_words.append(\"z\")\n",
    "# target_words.append(\"u\")\n",
    "# target_words.append(\"o\")\n",
    "# target_words.append(\"COMMA\")\n",
    "# target_words.append(\":\")\n",
    "# target_words.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 4586\n",
      "Number of unique output tokens: 8060\n",
      "Max sequence length for inputs: 6\n",
      "Max sequence length for outputs: 11\n"
     ]
    }
   ],
   "source": [
    "input_words = sorted(list(input_words))\n",
    "target_words = sorted(list(target_words))\n",
    "num_encoder_tokens = len(input_words)\n",
    "num_decoder_tokens = len(target_words)\n",
    "max_encoder_seq_length = max([len(txt.split()) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt.split()) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(input_words)\n",
    "# print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(input_texts)\n",
    "# print(target_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Make character-to-index dictionaries\n",
    "and prepare one-hot-encoding of the whole training input data.\n",
    "Note that the sequences are zero-(post)padded to have all equal lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(target_words)])\n",
    "\n",
    "x_train, y_train = input_texts[:int(len(input_texts)*0.9)], target_texts[:int(len(target_texts)*0.9)]\n",
    "x_test, y_test = input_texts[int(len(input_texts)*0.9):], target_texts[int(len(target_texts)*0.9):]\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(x_train), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "encoder_test_data = np.zeros(\n",
    "    (len(x_test), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "missed_e = 0\n",
    "missed_d = 0\n",
    "for i, (input_text, target_text) in enumerate(zip(x_train, y_train)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[word]   \n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(x_test, y_test)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_test_data[i, t] = input_token_index[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. Polish sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "en_x=  Embedding(num_encoder_tokens, embedding_size, input_length=max_encoder_seq_length)(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "A decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. It uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate `targets[t+1...]` given `targets[...t]`, conditioned on the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "dex=  Embedding(num_decoder_tokens, embedding_size)\n",
    "final_dex= dex(decoder_inputs)\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 50)     229300      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     403000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 314368      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  314368      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8060)   2071420     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,332,456\n",
      "Trainable params: 3,332,456\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "134/134 [==============================] - 25s 165ms/step - loss: 2.1129 - accuracy: 0.0941 - val_loss: 2.6857 - val_accuracy: 0.0915\n",
      "Epoch 2/200\n",
      "134/134 [==============================] - 21s 155ms/step - loss: 1.9771 - accuracy: 0.1005 - val_loss: 2.7529 - val_accuracy: 0.0931\n",
      "Epoch 3/200\n",
      "134/134 [==============================] - 20s 150ms/step - loss: 1.9026 - accuracy: 0.1054 - val_loss: 2.7599 - val_accuracy: 0.1000\n",
      "Epoch 4/200\n",
      "134/134 [==============================] - 19s 144ms/step - loss: 1.8477 - accuracy: 0.1107 - val_loss: 2.7624 - val_accuracy: 0.0964\n",
      "Epoch 5/200\n",
      "134/134 [==============================] - 19s 142ms/step - loss: 1.7986 - accuracy: 0.1141 - val_loss: 2.7522 - val_accuracy: 0.0964\n",
      "Epoch 6/200\n",
      "134/134 [==============================] - 20s 149ms/step - loss: 1.7650 - accuracy: 0.1153 - val_loss: 2.7471 - val_accuracy: 0.1022\n",
      "Epoch 7/200\n",
      "134/134 [==============================] - 20s 148ms/step - loss: 1.7369 - accuracy: 0.1171 - val_loss: 2.7248 - val_accuracy: 0.1028\n",
      "Epoch 8/200\n",
      "134/134 [==============================] - 19s 144ms/step - loss: 1.7114 - accuracy: 0.1181 - val_loss: 2.7268 - val_accuracy: 0.1101\n",
      "Epoch 9/200\n",
      "134/134 [==============================] - 19s 142ms/step - loss: 1.6885 - accuracy: 0.1205 - val_loss: 2.7066 - val_accuracy: 0.1123\n",
      "Epoch 10/200\n",
      "134/134 [==============================] - 18s 138ms/step - loss: 1.6665 - accuracy: 0.1223 - val_loss: 2.7470 - val_accuracy: 0.1111\n",
      "Epoch 11/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 1.6394 - accuracy: 0.1250 - val_loss: 2.7029 - val_accuracy: 0.1135\n",
      "Epoch 12/200\n",
      "134/134 [==============================] - 19s 141ms/step - loss: 1.6144 - accuracy: 0.1269 - val_loss: 2.6859 - val_accuracy: 0.1180\n",
      "Epoch 13/200\n",
      "134/134 [==============================] - 19s 138ms/step - loss: 1.5828 - accuracy: 0.1292 - val_loss: 2.6596 - val_accuracy: 0.1147\n",
      "Epoch 14/200\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 1.5498 - accuracy: 0.1313 - val_loss: 2.6486 - val_accuracy: 0.1200\n",
      "Epoch 15/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.5191 - accuracy: 0.1333 - val_loss: 2.6521 - val_accuracy: 0.1210\n",
      "Epoch 16/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 1.4858 - accuracy: 0.1355 - val_loss: 2.6327 - val_accuracy: 0.1222\n",
      "Epoch 17/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 1.4561 - accuracy: 0.1373 - val_loss: 2.6108 - val_accuracy: 0.1218\n",
      "Epoch 18/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 1.4233 - accuracy: 0.1392 - val_loss: 2.5686 - val_accuracy: 0.1279\n",
      "Epoch 19/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 1.3912 - accuracy: 0.1407 - val_loss: 2.5710 - val_accuracy: 0.1295\n",
      "Epoch 20/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 1.3612 - accuracy: 0.1429 - val_loss: 2.5761 - val_accuracy: 0.1291\n",
      "Epoch 21/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 1.3309 - accuracy: 0.1447 - val_loss: 2.5671 - val_accuracy: 0.1311\n",
      "Epoch 22/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 1.3000 - accuracy: 0.1460 - val_loss: 2.5580 - val_accuracy: 0.1339\n",
      "Epoch 23/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 1.2726 - accuracy: 0.1475 - val_loss: 2.5417 - val_accuracy: 0.1343\n",
      "Epoch 24/200\n",
      "134/134 [==============================] - 17s 131ms/step - loss: 1.2445 - accuracy: 0.1496 - val_loss: 2.5499 - val_accuracy: 0.1329\n",
      "Epoch 25/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 1.2178 - accuracy: 0.1517 - val_loss: 2.5253 - val_accuracy: 0.1335\n",
      "Epoch 26/200\n",
      "134/134 [==============================] - 19s 138ms/step - loss: 1.1918 - accuracy: 0.1530 - val_loss: 2.5082 - val_accuracy: 0.1370\n",
      "Epoch 27/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 1.1663 - accuracy: 0.1551 - val_loss: 2.5208 - val_accuracy: 0.1374\n",
      "Epoch 28/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 1.1424 - accuracy: 0.1568 - val_loss: 2.5097 - val_accuracy: 0.1378\n",
      "Epoch 29/200\n",
      "134/134 [==============================] - 18s 138ms/step - loss: 1.1179 - accuracy: 0.1582 - val_loss: 2.5250 - val_accuracy: 0.1404\n",
      "Epoch 30/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 1.0937 - accuracy: 0.1599 - val_loss: 2.5071 - val_accuracy: 0.1410\n",
      "Epoch 31/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 1.0738 - accuracy: 0.1613 - val_loss: 2.5236 - val_accuracy: 0.1408\n",
      "Epoch 32/200\n",
      "134/134 [==============================] - 18s 138ms/step - loss: 1.0532 - accuracy: 0.1630 - val_loss: 2.5209 - val_accuracy: 0.1388\n",
      "Epoch 33/200\n",
      "134/134 [==============================] - 18s 137ms/step - loss: 1.0318 - accuracy: 0.1645 - val_loss: 2.5562 - val_accuracy: 0.1414\n",
      "Epoch 34/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 1.0125 - accuracy: 0.1658 - val_loss: 2.5640 - val_accuracy: 0.1414\n",
      "Epoch 35/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 0.9943 - accuracy: 0.1673 - val_loss: 2.5626 - val_accuracy: 0.1438\n",
      "Epoch 36/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 0.9768 - accuracy: 0.1680 - val_loss: 2.6080 - val_accuracy: 0.1446\n",
      "Epoch 37/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 0.9611 - accuracy: 0.1703 - val_loss: 2.5935 - val_accuracy: 0.1434\n",
      "Epoch 38/200\n",
      "134/134 [==============================] - 18s 137ms/step - loss: 0.9462 - accuracy: 0.1714 - val_loss: 2.6326 - val_accuracy: 0.1446\n",
      "Epoch 39/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.9310 - accuracy: 0.1725 - val_loss: 2.6412 - val_accuracy: 0.1459\n",
      "Epoch 40/200\n",
      "134/134 [==============================] - 18s 136ms/step - loss: 0.9171 - accuracy: 0.1742 - val_loss: 2.6345 - val_accuracy: 0.1420\n",
      "Epoch 41/200\n",
      "134/134 [==============================] - 19s 145ms/step - loss: 0.9024 - accuracy: 0.1757 - val_loss: 2.6879 - val_accuracy: 0.1428\n",
      "Epoch 42/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 0.8892 - accuracy: 0.1770 - val_loss: 2.7033 - val_accuracy: 0.1438\n",
      "Epoch 43/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 0.8746 - accuracy: 0.1786 - val_loss: 2.7185 - val_accuracy: 0.1432\n",
      "Epoch 44/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.8623 - accuracy: 0.1799 - val_loss: 2.7259 - val_accuracy: 0.1451\n",
      "Epoch 45/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.8502 - accuracy: 0.1815 - val_loss: 2.7658 - val_accuracy: 0.1404\n",
      "Epoch 46/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.8366 - accuracy: 0.1832 - val_loss: 2.7641 - val_accuracy: 0.1428\n",
      "Epoch 47/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.8266 - accuracy: 0.1840 - val_loss: 2.8078 - val_accuracy: 0.1446\n",
      "Epoch 48/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.8164 - accuracy: 0.1855 - val_loss: 2.8231 - val_accuracy: 0.1451\n",
      "Epoch 49/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.8059 - accuracy: 0.1865 - val_loss: 2.8580 - val_accuracy: 0.1451\n",
      "Epoch 50/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.7977 - accuracy: 0.1879 - val_loss: 2.8893 - val_accuracy: 0.1438\n",
      "Epoch 51/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.7867 - accuracy: 0.1889 - val_loss: 2.9326 - val_accuracy: 0.1424\n",
      "Epoch 52/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.7770 - accuracy: 0.1901 - val_loss: 2.9486 - val_accuracy: 0.1420\n",
      "Epoch 53/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.7684 - accuracy: 0.1917 - val_loss: 2.9604 - val_accuracy: 0.1434\n",
      "Epoch 54/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.7595 - accuracy: 0.1926 - val_loss: 2.9484 - val_accuracy: 0.1446\n",
      "Epoch 55/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.7510 - accuracy: 0.1944 - val_loss: 2.9937 - val_accuracy: 0.1442\n",
      "Epoch 56/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.7420 - accuracy: 0.1953 - val_loss: 3.0057 - val_accuracy: 0.1446\n",
      "Epoch 57/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.7340 - accuracy: 0.1971 - val_loss: 3.0432 - val_accuracy: 0.1469\n",
      "Epoch 58/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.7244 - accuracy: 0.1974 - val_loss: 3.0583 - val_accuracy: 0.1434\n",
      "Epoch 59/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.7159 - accuracy: 0.1991 - val_loss: 3.0651 - val_accuracy: 0.1426\n",
      "Epoch 60/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.7085 - accuracy: 0.2002 - val_loss: 3.1247 - val_accuracy: 0.1463\n",
      "Epoch 61/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.6997 - accuracy: 0.2021 - val_loss: 3.1078 - val_accuracy: 0.1436\n",
      "Epoch 62/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.6940 - accuracy: 0.2025 - val_loss: 3.1618 - val_accuracy: 0.1430\n",
      "Epoch 63/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.6859 - accuracy: 0.2033 - val_loss: 3.1718 - val_accuracy: 0.1448\n",
      "Epoch 64/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.6788 - accuracy: 0.2047 - val_loss: 3.1947 - val_accuracy: 0.1448\n",
      "Epoch 65/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.6726 - accuracy: 0.2054 - val_loss: 3.2551 - val_accuracy: 0.1457\n",
      "Epoch 66/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.6666 - accuracy: 0.2060 - val_loss: 3.2506 - val_accuracy: 0.1455\n",
      "Epoch 67/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.6607 - accuracy: 0.2071 - val_loss: 3.3104 - val_accuracy: 0.1455\n",
      "Epoch 68/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.6523 - accuracy: 0.2087 - val_loss: 3.2964 - val_accuracy: 0.1471\n",
      "Epoch 69/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.6471 - accuracy: 0.2094 - val_loss: 3.3444 - val_accuracy: 0.1446\n",
      "Epoch 70/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.6415 - accuracy: 0.2104 - val_loss: 3.3424 - val_accuracy: 0.1442\n",
      "Epoch 71/200\n",
      "134/134 [==============================] - 18s 136ms/step - loss: 0.6351 - accuracy: 0.2115 - val_loss: 3.3617 - val_accuracy: 0.1465\n",
      "Epoch 72/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.6299 - accuracy: 0.2123 - val_loss: 3.4467 - val_accuracy: 0.1469\n",
      "Epoch 73/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.6239 - accuracy: 0.2136 - val_loss: 3.4526 - val_accuracy: 0.1416\n",
      "Epoch 74/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.6177 - accuracy: 0.2138 - val_loss: 3.4917 - val_accuracy: 0.1424\n",
      "Epoch 75/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.6110 - accuracy: 0.2155 - val_loss: 3.5117 - val_accuracy: 0.1473\n",
      "Epoch 76/200\n",
      "134/134 [==============================] - 17s 123ms/step - loss: 0.6051 - accuracy: 0.2159 - val_loss: 3.5305 - val_accuracy: 0.1473\n",
      "Epoch 77/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.6008 - accuracy: 0.2169 - val_loss: 3.5443 - val_accuracy: 0.1446\n",
      "Epoch 78/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.5950 - accuracy: 0.2179 - val_loss: 3.5414 - val_accuracy: 0.1465\n",
      "Epoch 79/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.5893 - accuracy: 0.2183 - val_loss: 3.5505 - val_accuracy: 0.1471\n",
      "Epoch 80/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.5839 - accuracy: 0.2194 - val_loss: 3.5712 - val_accuracy: 0.1457\n",
      "Epoch 81/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.5782 - accuracy: 0.2212 - val_loss: 3.6313 - val_accuracy: 0.1459\n",
      "Epoch 82/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.5738 - accuracy: 0.2210 - val_loss: 3.6649 - val_accuracy: 0.1481\n",
      "Epoch 83/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.5670 - accuracy: 0.2216 - val_loss: 3.6332 - val_accuracy: 0.1446\n",
      "Epoch 84/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.5628 - accuracy: 0.2233 - val_loss: 3.7039 - val_accuracy: 0.1455\n",
      "Epoch 85/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.5580 - accuracy: 0.2238 - val_loss: 3.7354 - val_accuracy: 0.1469\n",
      "Epoch 86/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.5533 - accuracy: 0.2243 - val_loss: 3.7632 - val_accuracy: 0.1483\n",
      "Epoch 87/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.5489 - accuracy: 0.2251 - val_loss: 3.7063 - val_accuracy: 0.1448\n",
      "Epoch 88/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.5435 - accuracy: 0.2258 - val_loss: 3.7214 - val_accuracy: 0.1428\n",
      "Epoch 89/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.5397 - accuracy: 0.2262 - val_loss: 3.8174 - val_accuracy: 0.1469\n",
      "Epoch 90/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.5350 - accuracy: 0.2272 - val_loss: 3.8580 - val_accuracy: 0.1465\n",
      "Epoch 91/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.5301 - accuracy: 0.2284 - val_loss: 3.8345 - val_accuracy: 0.1453\n",
      "Epoch 92/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.5274 - accuracy: 0.2288 - val_loss: 3.8374 - val_accuracy: 0.1453\n",
      "Epoch 93/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.5222 - accuracy: 0.2294 - val_loss: 3.8951 - val_accuracy: 0.1434\n",
      "Epoch 94/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.5188 - accuracy: 0.2301 - val_loss: 3.9403 - val_accuracy: 0.1436\n",
      "Epoch 95/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.5131 - accuracy: 0.2310 - val_loss: 3.9508 - val_accuracy: 0.1455\n",
      "Epoch 96/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.5070 - accuracy: 0.2320 - val_loss: 3.9882 - val_accuracy: 0.1301\n",
      "Epoch 97/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.5036 - accuracy: 0.2319 - val_loss: 3.9847 - val_accuracy: 0.1362\n",
      "Epoch 98/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.4993 - accuracy: 0.2329 - val_loss: 4.0075 - val_accuracy: 0.1418\n",
      "Epoch 99/200\n",
      "134/134 [==============================] - 17s 123ms/step - loss: 0.4956 - accuracy: 0.2339 - val_loss: 4.0291 - val_accuracy: 0.1451\n",
      "Epoch 100/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.4920 - accuracy: 0.2344 - val_loss: 4.1042 - val_accuracy: 0.1141\n",
      "Epoch 101/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.4877 - accuracy: 0.2347 - val_loss: 4.1108 - val_accuracy: 0.1414\n",
      "Epoch 102/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.4853 - accuracy: 0.2354 - val_loss: 4.1030 - val_accuracy: 0.1329\n",
      "Epoch 103/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.4803 - accuracy: 0.2365 - val_loss: 4.1608 - val_accuracy: 0.1349\n",
      "Epoch 104/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.4777 - accuracy: 0.2363 - val_loss: 4.1848 - val_accuracy: 0.1347\n",
      "Epoch 105/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.4743 - accuracy: 0.2380 - val_loss: 4.1453 - val_accuracy: 0.1444\n",
      "Epoch 106/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.4706 - accuracy: 0.2377 - val_loss: 4.2006 - val_accuracy: 0.1420\n",
      "Epoch 107/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.4678 - accuracy: 0.2388 - val_loss: 4.2339 - val_accuracy: 0.1455\n",
      "Epoch 108/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.4643 - accuracy: 0.2393 - val_loss: 4.2763 - val_accuracy: 0.1321\n",
      "Epoch 109/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.4607 - accuracy: 0.2396 - val_loss: 4.3478 - val_accuracy: 0.1152\n",
      "Epoch 110/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.4566 - accuracy: 0.2400 - val_loss: 4.3283 - val_accuracy: 0.1216\n",
      "Epoch 111/200\n",
      "134/134 [==============================] - 17s 123ms/step - loss: 0.4533 - accuracy: 0.2407 - val_loss: 4.3193 - val_accuracy: 0.1186\n",
      "Epoch 112/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.4491 - accuracy: 0.2413 - val_loss: 4.3879 - val_accuracy: 0.1325\n",
      "Epoch 113/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.4474 - accuracy: 0.2419 - val_loss: 4.3634 - val_accuracy: 0.1125\n",
      "Epoch 114/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.4436 - accuracy: 0.2421 - val_loss: 4.4119 - val_accuracy: 0.1113\n",
      "Epoch 115/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.4405 - accuracy: 0.2432 - val_loss: 4.4269 - val_accuracy: 0.1255\n",
      "Epoch 116/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.4385 - accuracy: 0.2436 - val_loss: 4.5005 - val_accuracy: 0.1200\n",
      "Epoch 117/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.4344 - accuracy: 0.2444 - val_loss: 4.4520 - val_accuracy: 0.1180\n",
      "Epoch 118/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.4315 - accuracy: 0.2441 - val_loss: 4.4689 - val_accuracy: 0.1115\n",
      "Epoch 119/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.4283 - accuracy: 0.2442 - val_loss: 4.5235 - val_accuracy: 0.1149\n",
      "Epoch 120/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.4260 - accuracy: 0.2454 - val_loss: 4.5846 - val_accuracy: 0.1156\n",
      "Epoch 121/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.4227 - accuracy: 0.2455 - val_loss: 4.6748 - val_accuracy: 0.1123\n",
      "Epoch 122/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.4212 - accuracy: 0.2464 - val_loss: 4.6049 - val_accuracy: 0.1178\n",
      "Epoch 123/200\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 0.4182 - accuracy: 0.2465 - val_loss: 4.6000 - val_accuracy: 0.1133\n",
      "Epoch 124/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.4148 - accuracy: 0.2470 - val_loss: 4.6929 - val_accuracy: 0.1164\n",
      "Epoch 125/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.4123 - accuracy: 0.2479 - val_loss: 4.6192 - val_accuracy: 0.1160\n",
      "Epoch 126/200\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 0.4100 - accuracy: 0.2478 - val_loss: 4.7003 - val_accuracy: 0.1137\n",
      "Epoch 127/200\n",
      "134/134 [==============================] - 16s 116ms/step - loss: 0.4064 - accuracy: 0.2484 - val_loss: 4.6918 - val_accuracy: 0.1160\n",
      "Epoch 128/200\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 0.4033 - accuracy: 0.2494 - val_loss: 4.6988 - val_accuracy: 0.1131\n",
      "Epoch 129/200\n",
      "134/134 [==============================] - 15s 115ms/step - loss: 0.4015 - accuracy: 0.2493 - val_loss: 4.8276 - val_accuracy: 0.1147\n",
      "Epoch 130/200\n",
      "134/134 [==============================] - 15s 114ms/step - loss: 0.3982 - accuracy: 0.2498 - val_loss: 4.8478 - val_accuracy: 0.1149\n",
      "Epoch 131/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.3966 - accuracy: 0.2501 - val_loss: 4.7744 - val_accuracy: 0.1133\n",
      "Epoch 132/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.3931 - accuracy: 0.2509 - val_loss: 4.7892 - val_accuracy: 0.1123\n",
      "Epoch 133/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.3905 - accuracy: 0.2520 - val_loss: 4.8381 - val_accuracy: 0.1141\n",
      "Epoch 134/200\n",
      "134/134 [==============================] - 16s 116ms/step - loss: 0.3889 - accuracy: 0.2511 - val_loss: 4.8206 - val_accuracy: 0.1147\n",
      "Epoch 135/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.3855 - accuracy: 0.2522 - val_loss: 4.9518 - val_accuracy: 0.1117\n",
      "Epoch 136/200\n",
      "134/134 [==============================] - 15s 115ms/step - loss: 0.3820 - accuracy: 0.2524 - val_loss: 4.9516 - val_accuracy: 0.1117\n",
      "Epoch 137/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.3788 - accuracy: 0.2530 - val_loss: 4.9961 - val_accuracy: 0.1129\n",
      "Epoch 138/200\n",
      "134/134 [==============================] - 15s 115ms/step - loss: 0.3780 - accuracy: 0.2540 - val_loss: 4.9952 - val_accuracy: 0.1117\n",
      "Epoch 139/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.3755 - accuracy: 0.2540 - val_loss: 5.0233 - val_accuracy: 0.1125\n",
      "Epoch 140/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.3730 - accuracy: 0.2542 - val_loss: 5.0102 - val_accuracy: 0.1109\n",
      "Epoch 141/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.3713 - accuracy: 0.2551 - val_loss: 5.0418 - val_accuracy: 0.1125\n",
      "Epoch 142/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.3673 - accuracy: 0.2551 - val_loss: 5.0984 - val_accuracy: 0.1103\n",
      "Epoch 143/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.3660 - accuracy: 0.2553 - val_loss: 5.1416 - val_accuracy: 0.1131\n",
      "Epoch 144/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.3629 - accuracy: 0.2562 - val_loss: 5.1081 - val_accuracy: 0.1129\n",
      "Epoch 145/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.3608 - accuracy: 0.2561 - val_loss: 5.0983 - val_accuracy: 0.1115\n",
      "Epoch 146/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.3565 - accuracy: 0.2573 - val_loss: 5.2831 - val_accuracy: 0.1129\n",
      "Epoch 147/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.3555 - accuracy: 0.2572 - val_loss: 5.1663 - val_accuracy: 0.1097\n",
      "Epoch 148/200\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 0.3529 - accuracy: 0.2573 - val_loss: 5.1884 - val_accuracy: 0.1091\n",
      "Epoch 149/200\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 0.3513 - accuracy: 0.2579 - val_loss: 5.2942 - val_accuracy: 0.1109\n",
      "Epoch 150/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.3477 - accuracy: 0.2583 - val_loss: 5.3000 - val_accuracy: 0.1099\n",
      "Epoch 151/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.3462 - accuracy: 0.2584 - val_loss: 5.2241 - val_accuracy: 0.1115\n",
      "Epoch 152/200\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 0.3445 - accuracy: 0.2585 - val_loss: 5.3518 - val_accuracy: 0.1131\n",
      "Epoch 153/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.3398 - accuracy: 0.2600 - val_loss: 5.3205 - val_accuracy: 0.1095\n",
      "Epoch 154/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.3392 - accuracy: 0.2593 - val_loss: 5.3172 - val_accuracy: 0.1145\n",
      "Epoch 155/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.3362 - accuracy: 0.2606 - val_loss: 5.3560 - val_accuracy: 0.1097\n",
      "Epoch 156/200\n",
      "134/134 [==============================] - 16s 116ms/step - loss: 0.3328 - accuracy: 0.2609 - val_loss: 5.4366 - val_accuracy: 0.1137\n",
      "Epoch 157/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.3317 - accuracy: 0.2608 - val_loss: 5.3553 - val_accuracy: 0.1121\n",
      "Epoch 158/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.3293 - accuracy: 0.2610 - val_loss: 5.4428 - val_accuracy: 0.1139\n",
      "Epoch 159/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.3270 - accuracy: 0.2619 - val_loss: 5.4757 - val_accuracy: 0.1113\n",
      "Epoch 160/200\n",
      "134/134 [==============================] - 17s 123ms/step - loss: 0.3241 - accuracy: 0.2619 - val_loss: 5.5198 - val_accuracy: 0.1099\n",
      "Epoch 161/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.3226 - accuracy: 0.2626 - val_loss: 5.4566 - val_accuracy: 0.1111\n",
      "Epoch 162/200\n",
      "134/134 [==============================] - 15s 116ms/step - loss: 0.3203 - accuracy: 0.2627 - val_loss: 5.4393 - val_accuracy: 0.1111\n",
      "Epoch 163/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.3182 - accuracy: 0.2628 - val_loss: 5.5156 - val_accuracy: 0.1117\n",
      "Epoch 164/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.3163 - accuracy: 0.2639 - val_loss: 5.6014 - val_accuracy: 0.1117\n",
      "Epoch 165/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.3152 - accuracy: 0.2633 - val_loss: 5.6191 - val_accuracy: 0.1139\n",
      "Epoch 166/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.3130 - accuracy: 0.2638 - val_loss: 5.5508 - val_accuracy: 0.1119\n",
      "Epoch 167/200\n",
      "134/134 [==============================] - 16s 116ms/step - loss: 0.3111 - accuracy: 0.2648 - val_loss: 5.5948 - val_accuracy: 0.1111\n",
      "Epoch 168/200\n",
      "134/134 [==============================] - 15s 113ms/step - loss: 0.3088 - accuracy: 0.2647 - val_loss: 5.6326 - val_accuracy: 0.1125\n",
      "Epoch 169/200\n",
      "134/134 [==============================] - 15s 114ms/step - loss: 0.3065 - accuracy: 0.2651 - val_loss: 5.6119 - val_accuracy: 0.1129\n",
      "Epoch 170/200\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 0.3033 - accuracy: 0.2658 - val_loss: 5.5943 - val_accuracy: 0.1129\n",
      "Epoch 171/200\n",
      "134/134 [==============================] - 15s 114ms/step - loss: 0.3004 - accuracy: 0.2660 - val_loss: 5.6705 - val_accuracy: 0.1109\n",
      "Epoch 172/200\n",
      "134/134 [==============================] - 15s 113ms/step - loss: 0.3003 - accuracy: 0.2657 - val_loss: 5.6427 - val_accuracy: 0.1115\n",
      "Epoch 173/200\n",
      "134/134 [==============================] - 15s 115ms/step - loss: 0.2979 - accuracy: 0.2667 - val_loss: 5.6608 - val_accuracy: 0.1115\n",
      "Epoch 174/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.2966 - accuracy: 0.2661 - val_loss: 5.7015 - val_accuracy: 0.1105\n",
      "Epoch 175/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.2943 - accuracy: 0.2676 - val_loss: 5.7363 - val_accuracy: 0.1119\n",
      "Epoch 176/200\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 0.2923 - accuracy: 0.2680 - val_loss: 5.7152 - val_accuracy: 0.1131\n",
      "Epoch 177/200\n",
      "134/134 [==============================] - 15s 112ms/step - loss: 0.2908 - accuracy: 0.2673 - val_loss: 5.7720 - val_accuracy: 0.1129\n",
      "Epoch 178/200\n",
      "134/134 [==============================] - 15s 113ms/step - loss: 0.2892 - accuracy: 0.2681 - val_loss: 5.7736 - val_accuracy: 0.1097\n",
      "Epoch 179/200\n",
      "134/134 [==============================] - 15s 113ms/step - loss: 0.2889 - accuracy: 0.2681 - val_loss: 5.7771 - val_accuracy: 0.1111\n",
      "Epoch 180/200\n",
      "134/134 [==============================] - 15s 109ms/step - loss: 0.2855 - accuracy: 0.2691 - val_loss: 5.8648 - val_accuracy: 0.1123\n",
      "Epoch 181/200\n",
      "134/134 [==============================] - 15s 112ms/step - loss: 0.2846 - accuracy: 0.2688 - val_loss: 5.8396 - val_accuracy: 0.1137\n",
      "Epoch 182/200\n",
      "134/134 [==============================] - 15s 110ms/step - loss: 0.2835 - accuracy: 0.2690 - val_loss: 5.7990 - val_accuracy: 0.1119\n",
      "Epoch 183/200\n",
      "134/134 [==============================] - 15s 112ms/step - loss: 0.2811 - accuracy: 0.2695 - val_loss: 5.8548 - val_accuracy: 0.1107\n",
      "Epoch 184/200\n",
      "134/134 [==============================] - 15s 109ms/step - loss: 0.2792 - accuracy: 0.2695 - val_loss: 5.8898 - val_accuracy: 0.1111\n",
      "Epoch 185/200\n",
      "134/134 [==============================] - 15s 111ms/step - loss: 0.2783 - accuracy: 0.2703 - val_loss: 5.8812 - val_accuracy: 0.1109\n",
      "Epoch 186/200\n",
      "134/134 [==============================] - 15s 111ms/step - loss: 0.2764 - accuracy: 0.2702 - val_loss: 5.8673 - val_accuracy: 0.1123\n",
      "Epoch 187/200\n",
      "134/134 [==============================] - 16s 116ms/step - loss: 0.2752 - accuracy: 0.2703 - val_loss: 5.9102 - val_accuracy: 0.1107\n",
      "Epoch 188/200\n",
      "134/134 [==============================] - 15s 114ms/step - loss: 0.2725 - accuracy: 0.2708 - val_loss: 6.0279 - val_accuracy: 0.1103\n",
      "Epoch 189/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.2730 - accuracy: 0.2712 - val_loss: 5.9371 - val_accuracy: 0.1123\n",
      "Epoch 190/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.2717 - accuracy: 0.2708 - val_loss: 6.0340 - val_accuracy: 0.1103\n",
      "Epoch 191/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.2685 - accuracy: 0.2716 - val_loss: 6.0163 - val_accuracy: 0.1123\n",
      "Epoch 192/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.2675 - accuracy: 0.2723 - val_loss: 6.0390 - val_accuracy: 0.1099\n",
      "Epoch 193/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.2671 - accuracy: 0.2722 - val_loss: 6.0113 - val_accuracy: 0.1143\n",
      "Epoch 194/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.2650 - accuracy: 0.2720 - val_loss: 6.0840 - val_accuracy: 0.1117\n",
      "Epoch 195/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.2636 - accuracy: 0.2722 - val_loss: 6.0349 - val_accuracy: 0.1107\n",
      "Epoch 196/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.2618 - accuracy: 0.2729 - val_loss: 6.0703 - val_accuracy: 0.1099\n",
      "Epoch 197/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.2596 - accuracy: 0.2732 - val_loss: 6.1111 - val_accuracy: 0.1087\n",
      "Epoch 198/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.2586 - accuracy: 0.2734 - val_loss: 6.1256 - val_accuracy: 0.1156\n",
      "Epoch 199/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.2593 - accuracy: 0.2732 - val_loss: 6.0243 - val_accuracy: 0.1121\n",
      "Epoch 200/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.2543 - accuracy: 0.2740 - val_loss: 6.0216 - val_accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.05)\n",
    "# Save model\n",
    "model.save('s2s-w.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In inference mode, when we want to decode unknown input sequences, we:\n",
    "- Encode the input sequence into state vectors and retrieve initial decoder state\n",
    "- Start with a target sequence of size 1 (just the start-of-sequence character)\n",
    "- Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character\n",
    "- Sample the next character using these predictions (we simply use argmax).\n",
    "- Append the sampled character to the target sequence\n",
    "- Repeat until we generate the end-of-sequence character or we hit the character limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 50)          229300    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 314368    \n",
      "=================================================================\n",
      "Total params: 543,668\n",
      "Trainable params: 543,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "final_dex2= dex(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "    final_dex2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['start_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_end' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence:  Idź. _end\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence:  Cześć. _end\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence:  Uciekaj! _end\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence:  Naprawdę? _end\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence:  Naprawdę? _end\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence:  Kto? _end\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence:  Łał! _end\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence:  Łał! _end\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence:  Unik! _end\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence:  Strzelaj! _end\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    # print('Input seq:', input_seq)\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: It started to snow.\n",
      "Decoded sentence:  To wygląda zdrowo.\n",
      "-\n",
      "Input sentence: It was a hot night.\n",
      "Decoded sentence:  To był gorący\n",
      "-\n",
      "Input sentence: It was just a joke.\n",
      "Decoded sentence:  To była tylko\n",
      "-\n",
      "Input sentence: It was only a joke.\n",
      "Decoded sentence:  To była tylko\n",
      "-\n",
      "Input sentence: It was quite windy.\n",
      "Decoded sentence:  To była magia.\n",
      "-\n",
      "Input sentence: It was spectacular.\n",
      "Decoded sentence:  To zależy od\n",
      "-\n",
      "Input sentence: It was very simple.\n",
      "Decoded sentence:  To było bardzo\n",
      "-\n",
      "Input sentence: It wasn't my fault.\n",
      "Decoded sentence:  Nie było mój\n",
      "-\n",
      "Input sentence: It wasn't that bad.\n",
      "Decoded sentence:  Nie było tak\n",
      "-\n",
      "Input sentence: It wasn't very fun.\n",
      "Decoded sentence:  To było moja\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_test_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    # print('Input seq:', input_seq)\n",
    "    print('Input sentence:', x_test[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
