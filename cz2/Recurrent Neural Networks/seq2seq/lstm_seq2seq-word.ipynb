{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence example in Keras (character-level).\n",
    "\n",
    "Source:\n",
    "https://keras.io/examples/lstm_seq2seq/\n",
    "\n",
    "References:\n",
    "- https://arxiv.org/abs/1409.3215\n",
    "- https://arxiv.org/abs/1406.1078\n",
    "\n",
    "This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short Polish sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download\n",
    "\n",
    "Lots of neat sentence pairs datasets. http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 200  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = '../lang_pairs/pol.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the sentences (input = English, target = Polish),\n",
    "and extract the character sets for each language.\n",
    "\n",
    "Target sequences get additional \"start sequence\" and \"end sequence\" characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_words = set()\n",
    "target_words = set()\n",
    "corpus = []\n",
    "# input_words.add(' ')\n",
    "# target_words.add(' ')\n",
    "# input_words.add('\\t')\n",
    "# target_words.add('\\t')\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    line = line.lower()\n",
    "    input_text, target_text, ignore = line.split('\\t') \n",
    "    input_text = re.sub(\"'\", '', input_text)\n",
    "    target_text = re.sub(\"'\", '', target_text)\n",
    "    input_text = re.sub(\",\", ' COMMA', input_text)\n",
    "    target_text = re.sub(\",\", ' COMMA', target_text)\n",
    "    input_text = re.sub(r'[?|!|.|\"|0-9]', r'', input_text)\n",
    "    target_text = re.sub(r'[?|!|.|\"|0-9]', r'', target_text)\n",
    "\n",
    "    target_text = 'start_ ' + target_text + ' _end'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # for word in input_text.split():\n",
    "    #     if word not in input_words:\n",
    "    #         input_words.add(word)\n",
    "    # for word in target_text.split():\n",
    "    #     if word not in target_words:\n",
    "    #         target_words.add(word)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer1 = CountVectorizer(lowercase=False)\n",
    "vectorizer2 = CountVectorizer(lowercase=False)\n",
    "\n",
    "X = vectorizer1.fit_transform(input_texts)\n",
    "Y = vectorizer2.fit_transform(target_texts)\n",
    "\n",
    "input_words = vectorizer1.get_feature_names()\n",
    "target_words = vectorizer2.get_feature_names()\n",
    "# target_words.append(\"START_\")\n",
    "# target_words.append(\"_END\")\n",
    "\n",
    "input_words.append(\"i\")\n",
    "input_words.append(\"a\")\n",
    "input_words.append(\"COMMA\")\n",
    "input_words.append(\":\")\n",
    "input_words.append(\"-\")\n",
    "input_words.append(\"b\")\n",
    "\n",
    "target_words.append(\"i\")\n",
    "target_words.append(\"a\")\n",
    "target_words.append(\"w\")\n",
    "target_words.append(\"z\")\n",
    "target_words.append(\"u\")\n",
    "target_words.append(\"o\")\n",
    "target_words.append(\"COMMA\")\n",
    "target_words.append(\":\")\n",
    "target_words.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 2996\n",
      "Number of unique output tokens: 5959\n",
      "Max sequence length for inputs: 6\n",
      "Max sequence length for outputs: 12\n"
     ]
    }
   ],
   "source": [
    "input_words = sorted(list(input_words))\n",
    "target_words = sorted(list(target_words))\n",
    "num_encoder_tokens = len(input_words)\n",
    "num_decoder_tokens = len(target_words)\n",
    "max_encoder_seq_length = max([len(txt.split()) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt.split()) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(input_words)\n",
    "# print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(input_texts)\n",
    "# print(target_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Make character-to-index dictionaries\n",
    "and prepare one-hot-encoding of the whole training input data.\n",
    "Note that the sequences are zero-(post)padded to have all equal lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(target_words)])\n",
    "\n",
    "x_train, y_train = input_texts[:int(len(input_texts)*0.9)], target_texts[:int(len(target_texts)*0.9)]\n",
    "x_test, y_test = input_texts[int(len(input_texts)*0.9):], target_texts[int(len(target_texts)*0.9):]\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(x_train), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "encoder_test_data = np.zeros(\n",
    "    (len(x_test), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "missed_e = 0\n",
    "missed_d = 0\n",
    "for i, (input_text, target_text) in enumerate(zip(x_train, y_train)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[word]   \n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(x_test, y_test)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_test_data[i, t] = input_token_index[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. Polish sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "en_x=  Embedding(num_encoder_tokens, embedding_size, input_length=max_encoder_seq_length)(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "A decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. It uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate `targets[t+1...]` given `targets[...t]`, conditioned on the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "dex=  Embedding(num_decoder_tokens, embedding_size)\n",
    "final_dex= dex(decoder_inputs)\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 50)     149800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     297950      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 314368      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  314368      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 5959)   1531463     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,607,949\n",
      "Trainable params: 2,607,949\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "134/134 [==============================] - 22s 143ms/step - loss: 1.8212 - accuracy: 0.0853 - val_loss: 2.3096 - val_accuracy: 0.0837\n",
      "Epoch 2/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 1.7270 - accuracy: 0.0903 - val_loss: 2.3523 - val_accuracy: 0.0870\n",
      "Epoch 3/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 1.6936 - accuracy: 0.0929 - val_loss: 2.3990 - val_accuracy: 0.0843\n",
      "Epoch 4/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 1.6674 - accuracy: 0.0957 - val_loss: 2.3843 - val_accuracy: 0.0885\n",
      "Epoch 5/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.6465 - accuracy: 0.0980 - val_loss: 2.3805 - val_accuracy: 0.0867\n",
      "Epoch 6/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 1.6298 - accuracy: 0.0993 - val_loss: 2.3804 - val_accuracy: 0.0865\n",
      "Epoch 7/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.6104 - accuracy: 0.1007 - val_loss: 2.3717 - val_accuracy: 0.0876\n",
      "Epoch 8/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.5805 - accuracy: 0.1021 - val_loss: 2.3468 - val_accuracy: 0.0898\n",
      "Epoch 9/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 1.5396 - accuracy: 0.1057 - val_loss: 2.3183 - val_accuracy: 0.0917\n",
      "Epoch 10/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.4960 - accuracy: 0.1105 - val_loss: 2.3073 - val_accuracy: 0.0983\n",
      "Epoch 11/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 1.4576 - accuracy: 0.1131 - val_loss: 2.2850 - val_accuracy: 0.1031\n",
      "Epoch 12/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.4211 - accuracy: 0.1151 - val_loss: 2.2619 - val_accuracy: 0.1070\n",
      "Epoch 13/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 1.3822 - accuracy: 0.1174 - val_loss: 2.2505 - val_accuracy: 0.1122\n",
      "Epoch 14/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.3466 - accuracy: 0.1211 - val_loss: 2.2437 - val_accuracy: 0.1115\n",
      "Epoch 15/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 1.3142 - accuracy: 0.1235 - val_loss: 2.2401 - val_accuracy: 0.1161\n",
      "Epoch 16/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.2839 - accuracy: 0.1258 - val_loss: 2.2357 - val_accuracy: 0.1157\n",
      "Epoch 17/200\n",
      "134/134 [==============================] - 19s 142ms/step - loss: 1.2565 - accuracy: 0.1282 - val_loss: 2.2339 - val_accuracy: 0.1176\n",
      "Epoch 18/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 1.2299 - accuracy: 0.1299 - val_loss: 2.2179 - val_accuracy: 0.1187\n",
      "Epoch 19/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.2054 - accuracy: 0.1318 - val_loss: 2.1963 - val_accuracy: 0.1211\n",
      "Epoch 20/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.1838 - accuracy: 0.1341 - val_loss: 2.2071 - val_accuracy: 0.1220\n",
      "Epoch 21/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.1624 - accuracy: 0.1358 - val_loss: 2.2067 - val_accuracy: 0.1278\n",
      "Epoch 22/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 1.1433 - accuracy: 0.1365 - val_loss: 2.2118 - val_accuracy: 0.1270\n",
      "Epoch 23/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 1.1242 - accuracy: 0.1382 - val_loss: 2.2227 - val_accuracy: 0.1231\n",
      "Epoch 24/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 1.1060 - accuracy: 0.1397 - val_loss: 2.2101 - val_accuracy: 0.1278\n",
      "Epoch 25/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.0903 - accuracy: 0.1409 - val_loss: 2.2367 - val_accuracy: 0.1272\n",
      "Epoch 26/200\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 1.0732 - accuracy: 0.1420 - val_loss: 2.2378 - val_accuracy: 0.1294\n",
      "Epoch 27/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.0583 - accuracy: 0.1433 - val_loss: 2.2253 - val_accuracy: 0.1270\n",
      "Epoch 28/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 1.0432 - accuracy: 0.1445 - val_loss: 2.2301 - val_accuracy: 0.1328\n",
      "Epoch 29/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 1.0287 - accuracy: 0.1460 - val_loss: 2.2506 - val_accuracy: 0.1324\n",
      "Epoch 30/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 1.0157 - accuracy: 0.1471 - val_loss: 2.2529 - val_accuracy: 0.1324\n",
      "Epoch 31/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 1.0045 - accuracy: 0.1479 - val_loss: 2.2862 - val_accuracy: 0.1324\n",
      "Epoch 32/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.9918 - accuracy: 0.1494 - val_loss: 2.2553 - val_accuracy: 0.1335\n",
      "Epoch 33/200\n",
      "134/134 [==============================] - 17s 131ms/step - loss: 0.9799 - accuracy: 0.1512 - val_loss: 2.2801 - val_accuracy: 0.1337\n",
      "Epoch 34/200\n",
      "134/134 [==============================] - 17s 131ms/step - loss: 0.9686 - accuracy: 0.1516 - val_loss: 2.2794 - val_accuracy: 0.1320\n",
      "Epoch 35/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 0.9578 - accuracy: 0.1524 - val_loss: 2.3040 - val_accuracy: 0.1317\n",
      "Epoch 36/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.9470 - accuracy: 0.1534 - val_loss: 2.3464 - val_accuracy: 0.1324\n",
      "Epoch 37/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.9376 - accuracy: 0.1549 - val_loss: 2.3167 - val_accuracy: 0.1346\n",
      "Epoch 38/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 0.9276 - accuracy: 0.1550 - val_loss: 2.3111 - val_accuracy: 0.1359\n",
      "Epoch 39/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.9178 - accuracy: 0.1562 - val_loss: 2.3117 - val_accuracy: 0.1335\n",
      "Epoch 40/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.9078 - accuracy: 0.1570 - val_loss: 2.3284 - val_accuracy: 0.1356\n",
      "Epoch 41/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 0.8992 - accuracy: 0.1582 - val_loss: 2.3491 - val_accuracy: 0.1363\n",
      "Epoch 42/200\n",
      "134/134 [==============================] - 17s 131ms/step - loss: 0.8901 - accuracy: 0.1587 - val_loss: 2.3320 - val_accuracy: 0.1346\n",
      "Epoch 43/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.8813 - accuracy: 0.1595 - val_loss: 2.3852 - val_accuracy: 0.1346\n",
      "Epoch 44/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.8725 - accuracy: 0.1604 - val_loss: 2.3489 - val_accuracy: 0.1356\n",
      "Epoch 45/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.8638 - accuracy: 0.1612 - val_loss: 2.3543 - val_accuracy: 0.1363\n",
      "Epoch 46/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.8556 - accuracy: 0.1624 - val_loss: 2.3894 - val_accuracy: 0.1350\n",
      "Epoch 47/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.8474 - accuracy: 0.1628 - val_loss: 2.4088 - val_accuracy: 0.1383\n",
      "Epoch 48/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.8389 - accuracy: 0.1636 - val_loss: 2.4121 - val_accuracy: 0.1376\n",
      "Epoch 49/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.8313 - accuracy: 0.1645 - val_loss: 2.4320 - val_accuracy: 0.1417\n",
      "Epoch 50/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.8233 - accuracy: 0.1656 - val_loss: 2.4235 - val_accuracy: 0.1393\n",
      "Epoch 51/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 0.8166 - accuracy: 0.1656 - val_loss: 2.4391 - val_accuracy: 0.1396\n",
      "Epoch 52/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.8066 - accuracy: 0.1670 - val_loss: 2.4762 - val_accuracy: 0.1365\n",
      "Epoch 53/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.7997 - accuracy: 0.1675 - val_loss: 2.4743 - val_accuracy: 0.1409\n",
      "Epoch 54/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.7911 - accuracy: 0.1687 - val_loss: 2.4750 - val_accuracy: 0.1411\n",
      "Epoch 55/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.7835 - accuracy: 0.1697 - val_loss: 2.4915 - val_accuracy: 0.1400\n",
      "Epoch 56/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.7762 - accuracy: 0.1707 - val_loss: 2.5023 - val_accuracy: 0.1419\n",
      "Epoch 57/200\n",
      "134/134 [==============================] - 17s 131ms/step - loss: 0.7683 - accuracy: 0.1712 - val_loss: 2.4876 - val_accuracy: 0.1431\n",
      "Epoch 58/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.7600 - accuracy: 0.1719 - val_loss: 2.5267 - val_accuracy: 0.1413\n",
      "Epoch 59/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.7539 - accuracy: 0.1735 - val_loss: 2.5422 - val_accuracy: 0.1428\n",
      "Epoch 60/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.7472 - accuracy: 0.1743 - val_loss: 2.5710 - val_accuracy: 0.1431\n",
      "Epoch 61/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 0.7398 - accuracy: 0.1752 - val_loss: 2.5776 - val_accuracy: 0.1446\n",
      "Epoch 62/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.7327 - accuracy: 0.1762 - val_loss: 2.6001 - val_accuracy: 0.1424\n",
      "Epoch 63/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.7271 - accuracy: 0.1762 - val_loss: 2.6288 - val_accuracy: 0.1452\n",
      "Epoch 64/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.7196 - accuracy: 0.1776 - val_loss: 2.6671 - val_accuracy: 0.1420\n",
      "Epoch 65/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.7130 - accuracy: 0.1784 - val_loss: 2.6258 - val_accuracy: 0.1459\n",
      "Epoch 66/200\n",
      "134/134 [==============================] - 17s 131ms/step - loss: 0.7059 - accuracy: 0.1795 - val_loss: 2.6550 - val_accuracy: 0.1441\n",
      "Epoch 67/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.7004 - accuracy: 0.1805 - val_loss: 2.6578 - val_accuracy: 0.1469\n",
      "Epoch 68/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.6939 - accuracy: 0.1811 - val_loss: 2.6696 - val_accuracy: 0.1437\n",
      "Epoch 69/200\n",
      "134/134 [==============================] - 18s 137ms/step - loss: 0.6872 - accuracy: 0.1815 - val_loss: 2.7014 - val_accuracy: 0.1444\n",
      "Epoch 70/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 0.6812 - accuracy: 0.1827 - val_loss: 2.6999 - val_accuracy: 0.1456\n",
      "Epoch 71/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.6742 - accuracy: 0.1832 - val_loss: 2.7562 - val_accuracy: 0.1426\n",
      "Epoch 72/200\n",
      "134/134 [==============================] - 17s 131ms/step - loss: 0.6676 - accuracy: 0.1846 - val_loss: 2.7172 - val_accuracy: 0.1456\n",
      "Epoch 73/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.6612 - accuracy: 0.1850 - val_loss: 2.7380 - val_accuracy: 0.1406\n",
      "Epoch 74/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.6552 - accuracy: 0.1858 - val_loss: 2.7658 - val_accuracy: 0.1448\n",
      "Epoch 75/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.6489 - accuracy: 0.1862 - val_loss: 2.7614 - val_accuracy: 0.1452\n",
      "Epoch 76/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.6431 - accuracy: 0.1874 - val_loss: 2.8140 - val_accuracy: 0.1450\n",
      "Epoch 77/200\n",
      "134/134 [==============================] - 43821s 329s/step - loss: 0.6378 - accuracy: 0.1880 - val_loss: 2.7833 - val_accuracy: 0.1454\n",
      "Epoch 78/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 0.6307 - accuracy: 0.1890 - val_loss: 2.7736 - val_accuracy: 0.1435\n",
      "Epoch 79/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.6256 - accuracy: 0.1894 - val_loss: 2.8092 - val_accuracy: 0.1441\n",
      "Epoch 80/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.6202 - accuracy: 0.1905 - val_loss: 2.8254 - val_accuracy: 0.1450\n",
      "Epoch 81/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.6137 - accuracy: 0.1912 - val_loss: 2.8262 - val_accuracy: 0.1452\n",
      "Epoch 82/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 0.6090 - accuracy: 0.1915 - val_loss: 2.8097 - val_accuracy: 0.1437\n",
      "Epoch 83/200\n",
      "134/134 [==============================] - 18s 136ms/step - loss: 0.6053 - accuracy: 0.1921 - val_loss: 2.8582 - val_accuracy: 0.1439\n",
      "Epoch 84/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.5989 - accuracy: 0.1923 - val_loss: 2.8521 - val_accuracy: 0.1441\n",
      "Epoch 85/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.5938 - accuracy: 0.1936 - val_loss: 2.8838 - val_accuracy: 0.1422\n",
      "Epoch 86/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.5896 - accuracy: 0.1938 - val_loss: 2.9112 - val_accuracy: 0.1444\n",
      "Epoch 87/200\n",
      "134/134 [==============================] - 18s 137ms/step - loss: 0.5830 - accuracy: 0.1950 - val_loss: 2.9234 - val_accuracy: 0.1456\n",
      "Epoch 88/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.5788 - accuracy: 0.1952 - val_loss: 2.9175 - val_accuracy: 0.1467\n",
      "Epoch 89/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.5757 - accuracy: 0.1965 - val_loss: 2.9131 - val_accuracy: 0.1441\n",
      "Epoch 90/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.5701 - accuracy: 0.1967 - val_loss: 2.9894 - val_accuracy: 0.1430\n",
      "Epoch 91/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.5667 - accuracy: 0.1974 - val_loss: 2.9586 - val_accuracy: 0.1469\n",
      "Epoch 92/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 0.5618 - accuracy: 0.1982 - val_loss: 3.0090 - val_accuracy: 0.1450\n",
      "Epoch 93/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.5573 - accuracy: 0.1983 - val_loss: 2.9675 - val_accuracy: 0.1454\n",
      "Epoch 94/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.5503 - accuracy: 0.1987 - val_loss: 3.0061 - val_accuracy: 0.1431\n",
      "Epoch 95/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 0.5452 - accuracy: 0.2005 - val_loss: 2.9924 - val_accuracy: 0.1463\n",
      "Epoch 96/200\n",
      "134/134 [==============================] - 17s 131ms/step - loss: 0.5399 - accuracy: 0.2009 - val_loss: 3.0344 - val_accuracy: 0.1448\n",
      "Epoch 97/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.5350 - accuracy: 0.2011 - val_loss: 3.0438 - val_accuracy: 0.1459\n",
      "Epoch 98/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.5322 - accuracy: 0.2019 - val_loss: 3.0700 - val_accuracy: 0.1459\n",
      "Epoch 99/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.5274 - accuracy: 0.2024 - val_loss: 3.0856 - val_accuracy: 0.1435\n",
      "Epoch 100/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.5225 - accuracy: 0.2033 - val_loss: 3.0995 - val_accuracy: 0.1444\n",
      "Epoch 101/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.5181 - accuracy: 0.2042 - val_loss: 3.0513 - val_accuracy: 0.1417\n",
      "Epoch 102/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 0.5147 - accuracy: 0.2045 - val_loss: 3.1417 - val_accuracy: 0.1448\n",
      "Epoch 103/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.5094 - accuracy: 0.2049 - val_loss: 3.1118 - val_accuracy: 0.1439\n",
      "Epoch 104/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.5052 - accuracy: 0.2062 - val_loss: 3.1610 - val_accuracy: 0.1435\n",
      "Epoch 105/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.5009 - accuracy: 0.2057 - val_loss: 3.1428 - val_accuracy: 0.1428\n",
      "Epoch 106/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.4952 - accuracy: 0.2075 - val_loss: 3.1393 - val_accuracy: 0.1433\n",
      "Epoch 107/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.4915 - accuracy: 0.2075 - val_loss: 3.1478 - val_accuracy: 0.1413\n",
      "Epoch 108/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.4878 - accuracy: 0.2084 - val_loss: 3.1990 - val_accuracy: 0.1424\n",
      "Epoch 109/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.4828 - accuracy: 0.2089 - val_loss: 3.2047 - val_accuracy: 0.1413\n",
      "Epoch 110/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.4792 - accuracy: 0.2097 - val_loss: 3.1931 - val_accuracy: 0.1430\n",
      "Epoch 111/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.4747 - accuracy: 0.2100 - val_loss: 3.1866 - val_accuracy: 0.1413\n",
      "Epoch 112/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.4704 - accuracy: 0.2105 - val_loss: 3.2438 - val_accuracy: 0.1393\n",
      "Epoch 113/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.4652 - accuracy: 0.2113 - val_loss: 3.2192 - val_accuracy: 0.1430\n",
      "Epoch 114/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.4619 - accuracy: 0.2119 - val_loss: 3.2758 - val_accuracy: 0.1363\n",
      "Epoch 115/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.4556 - accuracy: 0.2126 - val_loss: 3.2577 - val_accuracy: 0.1404\n",
      "Epoch 116/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.4517 - accuracy: 0.2133 - val_loss: 3.2120 - val_accuracy: 0.1413\n",
      "Epoch 117/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.4466 - accuracy: 0.2143 - val_loss: 3.2450 - val_accuracy: 0.1406\n",
      "Epoch 118/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.4443 - accuracy: 0.2144 - val_loss: 3.1659 - val_accuracy: 0.1404\n",
      "Epoch 119/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.4384 - accuracy: 0.2148 - val_loss: 3.2093 - val_accuracy: 0.1389\n",
      "Epoch 120/200\n",
      "134/134 [==============================] - 17s 131ms/step - loss: 0.4340 - accuracy: 0.2154 - val_loss: 3.2104 - val_accuracy: 0.1369\n",
      "Epoch 121/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.4295 - accuracy: 0.2164 - val_loss: 3.2474 - val_accuracy: 0.1406\n",
      "Epoch 122/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.4258 - accuracy: 0.2164 - val_loss: 3.2637 - val_accuracy: 0.1394\n",
      "Epoch 123/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.4211 - accuracy: 0.2175 - val_loss: 3.2485 - val_accuracy: 0.1391\n",
      "Epoch 124/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.4178 - accuracy: 0.2182 - val_loss: 3.2936 - val_accuracy: 0.1406\n",
      "Epoch 125/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.4131 - accuracy: 0.2184 - val_loss: 3.3242 - val_accuracy: 0.1367\n",
      "Epoch 126/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.4101 - accuracy: 0.2190 - val_loss: 3.2978 - val_accuracy: 0.1363\n",
      "Epoch 127/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.4072 - accuracy: 0.2196 - val_loss: 3.2814 - val_accuracy: 0.1376\n",
      "Epoch 128/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.4021 - accuracy: 0.2200 - val_loss: 3.3309 - val_accuracy: 0.1378\n",
      "Epoch 129/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.3968 - accuracy: 0.2216 - val_loss: 3.3581 - val_accuracy: 0.1391\n",
      "Epoch 130/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.3937 - accuracy: 0.2217 - val_loss: 3.3291 - val_accuracy: 0.1372\n",
      "Epoch 131/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.3893 - accuracy: 0.2219 - val_loss: 3.3697 - val_accuracy: 0.1361\n",
      "Epoch 132/200\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 0.3874 - accuracy: 0.2227 - val_loss: 3.3667 - val_accuracy: 0.1396\n",
      "Epoch 133/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.3822 - accuracy: 0.2239 - val_loss: 3.3664 - val_accuracy: 0.1411\n",
      "Epoch 134/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.3796 - accuracy: 0.2241 - val_loss: 3.3844 - val_accuracy: 0.1422\n",
      "Epoch 135/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.3757 - accuracy: 0.2247 - val_loss: 3.3692 - val_accuracy: 0.1415\n",
      "Epoch 136/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.3728 - accuracy: 0.2252 - val_loss: 3.3967 - val_accuracy: 0.1350\n",
      "Epoch 137/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.3692 - accuracy: 0.2256 - val_loss: 3.3852 - val_accuracy: 0.1398\n",
      "Epoch 138/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.3655 - accuracy: 0.2268 - val_loss: 3.4271 - val_accuracy: 0.1369\n",
      "Epoch 139/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.3618 - accuracy: 0.2269 - val_loss: 3.4968 - val_accuracy: 0.1320\n",
      "Epoch 140/200\n",
      "134/134 [==============================] - 18s 136ms/step - loss: 0.3585 - accuracy: 0.2282 - val_loss: 3.4831 - val_accuracy: 0.1369\n",
      "Epoch 141/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.3555 - accuracy: 0.2278 - val_loss: 3.4708 - val_accuracy: 0.1389\n",
      "Epoch 142/200\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 0.3520 - accuracy: 0.2292 - val_loss: 3.5129 - val_accuracy: 0.1378\n",
      "Epoch 143/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 0.3503 - accuracy: 0.2291 - val_loss: 3.4790 - val_accuracy: 0.1320\n",
      "Epoch 144/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.3463 - accuracy: 0.2299 - val_loss: 3.4937 - val_accuracy: 0.1372\n",
      "Epoch 145/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.3428 - accuracy: 0.2306 - val_loss: 3.5807 - val_accuracy: 0.1369\n",
      "Epoch 146/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.3413 - accuracy: 0.2306 - val_loss: 3.5345 - val_accuracy: 0.1365\n",
      "Epoch 147/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.3382 - accuracy: 0.2308 - val_loss: 3.5599 - val_accuracy: 0.1339\n",
      "Epoch 148/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.3351 - accuracy: 0.2320 - val_loss: 3.6378 - val_accuracy: 0.1361\n",
      "Epoch 149/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.3332 - accuracy: 0.2323 - val_loss: 3.5779 - val_accuracy: 0.1372\n",
      "Epoch 150/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.3301 - accuracy: 0.2331 - val_loss: 3.6104 - val_accuracy: 0.1359\n",
      "Epoch 151/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.3274 - accuracy: 0.2331 - val_loss: 3.6225 - val_accuracy: 0.1370\n",
      "Epoch 152/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.3248 - accuracy: 0.2333 - val_loss: 3.6141 - val_accuracy: 0.1374\n",
      "Epoch 153/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.3215 - accuracy: 0.2347 - val_loss: 3.6258 - val_accuracy: 0.1394\n",
      "Epoch 154/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.3191 - accuracy: 0.2347 - val_loss: 3.6635 - val_accuracy: 0.1387\n",
      "Epoch 155/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 0.3161 - accuracy: 0.2352 - val_loss: 3.6444 - val_accuracy: 0.1344\n",
      "Epoch 156/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.3129 - accuracy: 0.2360 - val_loss: 3.6780 - val_accuracy: 0.1374\n",
      "Epoch 157/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.3101 - accuracy: 0.2361 - val_loss: 3.6660 - val_accuracy: 0.1357\n",
      "Epoch 158/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.3074 - accuracy: 0.2363 - val_loss: 3.7660 - val_accuracy: 0.1374\n",
      "Epoch 159/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.3048 - accuracy: 0.2379 - val_loss: 3.7135 - val_accuracy: 0.1376\n",
      "Epoch 160/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.3019 - accuracy: 0.2378 - val_loss: 3.7549 - val_accuracy: 0.1346\n",
      "Epoch 161/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.3001 - accuracy: 0.2382 - val_loss: 3.7664 - val_accuracy: 0.1391\n",
      "Epoch 162/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.2980 - accuracy: 0.2384 - val_loss: 3.7758 - val_accuracy: 0.1356\n",
      "Epoch 163/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 0.2959 - accuracy: 0.2391 - val_loss: 3.8031 - val_accuracy: 0.1346\n",
      "Epoch 164/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 0.2920 - accuracy: 0.2397 - val_loss: 3.8131 - val_accuracy: 0.1356\n",
      "Epoch 165/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.2892 - accuracy: 0.2397 - val_loss: 3.8641 - val_accuracy: 0.1352\n",
      "Epoch 166/200\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 0.2865 - accuracy: 0.2397 - val_loss: 3.8129 - val_accuracy: 0.1315\n",
      "Epoch 167/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.2824 - accuracy: 0.2414 - val_loss: 3.8111 - val_accuracy: 0.1339\n",
      "Epoch 168/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.2804 - accuracy: 0.2417 - val_loss: 3.8588 - val_accuracy: 0.1328\n",
      "Epoch 169/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.2788 - accuracy: 0.2418 - val_loss: 3.8991 - val_accuracy: 0.1307\n",
      "Epoch 170/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.2761 - accuracy: 0.2424 - val_loss: 3.8863 - val_accuracy: 0.1352\n",
      "Epoch 171/200\n",
      "134/134 [==============================] - 16s 122ms/step - loss: 0.2740 - accuracy: 0.2433 - val_loss: 3.8770 - val_accuracy: 0.1344\n",
      "Epoch 172/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.2727 - accuracy: 0.2429 - val_loss: 3.9277 - val_accuracy: 0.1317\n",
      "Epoch 173/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.2684 - accuracy: 0.2441 - val_loss: 3.9304 - val_accuracy: 0.1296\n",
      "Epoch 174/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.2668 - accuracy: 0.2438 - val_loss: 3.9157 - val_accuracy: 0.1311\n",
      "Epoch 175/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.2639 - accuracy: 0.2445 - val_loss: 3.9437 - val_accuracy: 0.1320\n",
      "Epoch 176/200\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 0.2628 - accuracy: 0.2452 - val_loss: 3.9630 - val_accuracy: 0.1307\n",
      "Epoch 177/200\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 0.2598 - accuracy: 0.2456 - val_loss: 3.9925 - val_accuracy: 0.1326\n",
      "Epoch 178/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.2568 - accuracy: 0.2459 - val_loss: 3.9738 - val_accuracy: 0.1344\n",
      "Epoch 179/200\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 0.2530 - accuracy: 0.2466 - val_loss: 3.9775 - val_accuracy: 0.1346\n",
      "Epoch 180/200\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 0.2529 - accuracy: 0.2470 - val_loss: 4.0005 - val_accuracy: 0.1304\n",
      "Epoch 181/200\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 0.2493 - accuracy: 0.2471 - val_loss: 4.0254 - val_accuracy: 0.1319\n",
      "Epoch 182/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.2476 - accuracy: 0.2475 - val_loss: 4.0703 - val_accuracy: 0.1330\n",
      "Epoch 183/200\n",
      "134/134 [==============================] - 16s 123ms/step - loss: 0.2460 - accuracy: 0.2481 - val_loss: 4.0819 - val_accuracy: 0.1313\n",
      "Epoch 184/200\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 0.2435 - accuracy: 0.2483 - val_loss: 4.0600 - val_accuracy: 0.1335\n",
      "Epoch 185/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.2410 - accuracy: 0.2487 - val_loss: 4.0929 - val_accuracy: 0.1348\n",
      "Epoch 186/200\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 0.2398 - accuracy: 0.2485 - val_loss: 4.0672 - val_accuracy: 0.1339\n",
      "Epoch 187/200\n",
      "134/134 [==============================] - 17s 130ms/step - loss: 0.2358 - accuracy: 0.2503 - val_loss: 4.1540 - val_accuracy: 0.1306\n",
      "Epoch 188/200\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 0.2358 - accuracy: 0.2495 - val_loss: 4.1567 - val_accuracy: 0.1304\n",
      "Epoch 189/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.2329 - accuracy: 0.2505 - val_loss: 4.1538 - val_accuracy: 0.1339\n",
      "Epoch 190/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.2311 - accuracy: 0.2507 - val_loss: 4.2268 - val_accuracy: 0.1244\n",
      "Epoch 191/200\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 0.2275 - accuracy: 0.2512 - val_loss: 4.2632 - val_accuracy: 0.1289\n",
      "Epoch 192/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.2252 - accuracy: 0.2520 - val_loss: 4.2705 - val_accuracy: 0.1287\n",
      "Epoch 193/200\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 0.2244 - accuracy: 0.2515 - val_loss: 4.2291 - val_accuracy: 0.1276\n",
      "Epoch 194/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.2224 - accuracy: 0.2523 - val_loss: 4.2024 - val_accuracy: 0.1326\n",
      "Epoch 195/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.2193 - accuracy: 0.2531 - val_loss: 4.2746 - val_accuracy: 0.1306\n",
      "Epoch 196/200\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 0.2178 - accuracy: 0.2532 - val_loss: 4.3233 - val_accuracy: 0.1233\n",
      "Epoch 197/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.2164 - accuracy: 0.2533 - val_loss: 4.3735 - val_accuracy: 0.1259\n",
      "Epoch 198/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.2133 - accuracy: 0.2540 - val_loss: 4.2237 - val_accuracy: 0.1309\n",
      "Epoch 199/200\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 0.2119 - accuracy: 0.2548 - val_loss: 4.2751 - val_accuracy: 0.1317\n",
      "Epoch 200/200\n",
      "134/134 [==============================] - 17s 126ms/step - loss: 0.2096 - accuracy: 0.2542 - val_loss: 4.3528 - val_accuracy: 0.1274\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.05)\n",
    "# Save model\n",
    "model.save('s2s-w.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In inference mode, when we want to decode unknown input sequences, we:\n",
    "- Encode the input sequence into state vectors and retrieve initial decoder state\n",
    "- Start with a target sequence of size 1 (just the start-of-sequence character)\n",
    "- Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character\n",
    "- Sample the next character using these predictions (we simply use argmax).\n",
    "- Append the sampled character to the target sequence\n",
    "- Repeat until we generate the end-of-sequence character or we hit the character limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 50)          149800    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 314368    \n",
      "=================================================================\n",
      "Total params: 464,168\n",
      "Trainable params: 464,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "final_dex2= dex(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "    final_dex2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['start_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_end' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: go\n",
      "Decoded sentence:  proszę _end\n",
      "-\n",
      "Input sentence: hi\n",
      "Decoded sentence:  cześć _end\n",
      "-\n",
      "Input sentence: run\n",
      "Decoded sentence:  uciekaj _end\n",
      "-\n",
      "Input sentence: run\n",
      "Decoded sentence:  uciekaj _end\n",
      "-\n",
      "Input sentence: run\n",
      "Decoded sentence:  uciekaj _end\n",
      "-\n",
      "Input sentence: who\n",
      "Decoded sentence:  kto _end\n",
      "-\n",
      "Input sentence: wow\n",
      "Decoded sentence:  łał _end\n",
      "-\n",
      "Input sentence: wow\n",
      "Decoded sentence:  łał _end\n",
      "-\n",
      "Input sentence: duck\n",
      "Decoded sentence:  unik _end\n",
      "-\n",
      "Input sentence: fire\n",
      "Decoded sentence:  strzelaj _end\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    # print('Input seq:', input_seq)\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: it started to snow\n",
      "Decoded sentence:  to wymaga _end\n",
      "-\n",
      "Input sentence: it was a hot night\n",
      "Decoded sentence:  była zdrowia\n",
      "-\n",
      "Input sentence: it was just a joke\n",
      "Decoded sentence:  to była tylko\n",
      "-\n",
      "Input sentence: it was only a joke\n",
      "Decoded sentence:  to była tylko\n",
      "-\n",
      "Input sentence: it was quite windy\n",
      "Decoded sentence:  była był czarny\n",
      "-\n",
      "Input sentence: it was spectacular\n",
      "Decoded sentence:  to było kłamstwo\n",
      "-\n",
      "Input sentence: it was very simple\n",
      "Decoded sentence:  to było bardzo\n",
      "-\n",
      "Input sentence: it wasnt my fault\n",
      "Decoded sentence:  to nie naszego\n",
      "-\n",
      "Input sentence: it wasnt that bad\n",
      "Decoded sentence:  to nie jest tak\n",
      "-\n",
      "Input sentence: it wasnt very fun\n",
      "Decoded sentence:  to nie było pies\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_test_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    # print('Input seq:', input_seq)\n",
    "    print('Input sentence:', x_test[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
